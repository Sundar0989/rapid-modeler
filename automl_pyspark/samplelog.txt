Pulling image us-central1-docker.pkg.dev/atus-prism-dev/ml-repo/rapid_modeler_dataproc:latest
Image is up to date for sha256:c562dfe97cdf1078d319af362186542e70ac5d05c455e9d10289e48fa1f04d81
Waiting for container log creation
PYSPARK_PYTHON=python3
JAVA_HOME=/usr/lib/jvm/temurin-17-jdk-amd64
SPARK_EXTRA_CLASSPATH=
:: loading settings :: file = /etc/spark/conf/ivysettings.xml
[2025-09-24T21:38:42.115288] üõ°Ô∏è Cleanup handlers registered
[2025-09-24T21:38:42.115380] ‚è∞ Maximum timeout protection set: 180 minutes
[2025-09-24T21:38:42.116061] üöÄ Starting Dataproc Serverless job: job_0002_automl_user_automl_model_1758749709
[2025-09-24T21:38:42.116139] üìä Batch ID: automl-spark-job-0002-automl-user-automl-model-1758749709-20250
[2025-09-24T21:38:42.116169] üìÅ Working directory: /var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673
[2025-09-24T21:38:42.116200] üìä Progress: 12.5% - Initializing Spark session...
[2025-09-24T21:39:16.569343] üß† Spark resource allocation:
[2025-09-24T21:39:18.573508]    Executors: 8
[2025-09-24T21:39:18.574178]    Executor Memory: 32g
[2025-09-24T21:39:18.574891]    Driver Memory: 16g
[2025-09-24T21:39:18.574921] ‚úÖ Spark session initialized for Dataproc Serverless
[2025-09-24T21:39:18.577144] üìä Spark UI available at: http://gdpic-srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673-m.us-east1-d.c.atus-prism-dev.internal:4040
[2025-09-24T21:39:18.578824] üîß Spark configuration: driver.memory=16g, executor.memory=32g
[2025-09-24T21:39:18.578863] üì¶ Using custom container image with pre-installed packages...
[2025-09-24T21:39:18.578872] ‚ÑπÔ∏è Skipping runtime package installation - all dependencies are pre-installed
[2025-09-24T21:39:18.580243] ‚úÖ Config file found at: /app/automl_pyspark/config.yaml
[2025-09-24T21:39:18.581322] üìÅ Results will be saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
[2025-09-24T21:39:18.582144] üîç Debugging mounted directories:
[2025-09-24T21:39:18.583351] üìÇ /app/automl_pyspark/automl_results/job_0002_automl_user_automl_model_1758749709 does not exist yet
[2025-09-24T21:39:18.584202] üìÇ Active results_dir /tmp/automl_results/job_0002_automl_user_automl_model_1758749709 contains: []
[2025-09-24T21:39:18.584957] üìä Progress: 25.0% - Importing AutoML classes...
[2025-09-24T21:39:18.585244] üì¶ Importing AutoML classes...
‚úÖ XGBoost Spark integration available and functional
‚úÖ Native LightGBM integration available and functional (version: 4.6.0)
‚úÖ Native LightGBM regression available and functional (version: 4.6.0)
[2025-09-24T21:39:25.441587] ‚úÖ AutoML classes imported successfully
[2025-09-24T21:39:25.441631] üîß Applying Dataproc compatibility fixes...
[2025-09-24T21:39:25.441646] ‚úÖ Applied pandas Excel fallback patch
[2025-09-24T21:39:27.427511] ‚úÖ SHAP library is available
[2025-09-24T21:39:27.427551] ‚úÖ Dataproc compatibility fixes applied
[2025-09-24T21:39:27.427571] üìä Progress: 37.5% - Initializing AutoML...
[2025-09-24T21:39:27.427583] üìÅ Using original data file path: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
[2025-09-24T21:39:27.427590] üìÅ Data file: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
[2025-09-24T21:39:27.427596] üéØ Target column: target
[2025-09-24T21:39:27.427602] üìã Task type: classification
[2025-09-24T21:39:27.427608] üìã Source type: bigquery
[2025-09-24T21:39:27.427613] üèóÔ∏è Initializing AutoML Class...
[2025-09-24T21:39:27.427639] üîß Using config file: /app/automl_pyspark/config.yaml
‚úÖ Configuration loaded from: /app/automl_pyspark/config.yaml
üåç Using environment: development
‚úÖ Applying development environment configuration
‚úÖ XGBoost detected and available
‚úÖ Native LightGBM detected and available (version: 4.6.0)
‚úÖ Set run_xgboost to True for regression
‚úÖ Set run_lightgbm to True for regression
üéØ Applying '' preset configuration (highest priority)...
   üìù No preset selected - using configuration from YAML file
‚úÖ XGBoost integration verified - adding to available models
‚úÖ Native LightGBM integration verified - adding to available models

üîß AutoML Configuration Loaded:
   üìÅ Config source: /app/automl_pyspark/config.yaml
‚úÖ Configuration validation passed for classification

üîß AutoML Model Configuration:
  Core Models:
    Logistic Regression: ‚úÖ Enabled
    Random Forest: ‚≠ï Disabled
    Gradient Boosting: ‚≠ï Disabled
    Decision Tree: ‚≠ï Disabled
    Neural Network: ‚≠ï Disabled
  Advanced Models:
    XGBoost: ‚úÖ Enabled
    LightGBM: ‚≠ï Disabled
  Total Models Enabled: 2
  üí° Install XGBoost: pip install xgboost>=1.6.0
  üí° Install LightGBM: pip install synapseml>=0.11.0
[2025-09-24T21:39:27.482855] ‚úÖ AutoML initialized successfully
[2025-09-24T21:39:27.482901] üìä Progress: 50.0% - Initializing data manager...
[2025-09-24T21:39:27.495213] ‚úÖ Data manager initialized successfully
[2025-09-24T21:39:27.495281] üìä Progress: 62.5% - Running AutoML pipeline...
[2025-09-24T21:39:27.495290] üéØ Starting classification AutoML pipeline...
[2025-09-24T21:39:27.495300] üìÖ Loading OOT1 data from BigQuery: `atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism_oot1`
‚ö†Ô∏è BigQuery data loading attempt 1/3 failed: An error occurred while calling o96.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚è≥ Retrying in 5 seconds...
‚ö†Ô∏è BigQuery data loading attempt 2/3 failed: An error occurred while calling o106.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚è≥ Retrying in 10 seconds...
‚ö†Ô∏è BigQuery data loading attempt 3/3 failed: An error occurred while calling o116.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚ùå All 3 attempts failed
[2025-09-24T21:39:53.223024] ‚ö†Ô∏è OOT1 BigQuery loading failed: Failed to load data from BigQuery after 3 attempts. Last error: An error occurred while calling o116.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more
 - will skip OOT1
[2025-09-24T21:39:53.223079] üìÖ Loading OOT2 data from BigQuery: `atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism_oot2`
‚ö†Ô∏è BigQuery data loading attempt 1/3 failed: An error occurred while calling o126.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚è≥ Retrying in 5 seconds...
‚ö†Ô∏è BigQuery data loading attempt 2/3 failed: An error occurred while calling o136.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚è≥ Retrying in 10 seconds...
‚ö†Ô∏è BigQuery data loading attempt 3/3 failed: An error occurred while calling o146.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

‚ùå All 3 attempts failed
[2025-09-24T21:40:08.532210] ‚ö†Ô∏è OOT2 BigQuery loading failed: Failed to load data from BigQuery after 3 attempts. Last error: An error occurred while calling o146.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more
 - will skip OOT2
[2025-09-24T21:40:08.532264] üîÑ OOT1 data is None - will not be passed to AutoML.fit()
[2025-09-24T21:40:08.532277] üîÑ OOT2 data is None - will not be passed to AutoML.fit()
[2025-09-24T21:40:08.533095] üîÑ Calling AutoML.fit() with 28 parameters...
üìä Loading training data from: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
üîç Large dataset detected: 2,596,745 rows
üìä Creating sample temp table (100K rows) for feature engineering...
üîß Creating sample temp table with query: 
            CREATE TABLE `atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc` AS
    ...
‚úÖ Sample temp table created: atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc (expires in 24h)
‚úÖ Using sample temp table for feature engineering: atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc
25/09/24 21:40:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
25/09/24 21:40:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:40:37 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:37 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üìä Loaded sampled data for feature engineering: 100,000 rows
üí° Full dataset will be loaded after feature selection for training
‚è±Ô∏è Pipeline timeout set to 120 minutes
‚è±Ô∏è Setting pipeline timeout: 120 minutes

üîß AutoML Model Configuration:
  Core Models:
    Logistic Regression: ‚≠ï Disabled
    Random Forest: ‚≠ï Disabled
    Gradient Boosting: ‚≠ï Disabled
    Decision Tree: ‚≠ï Disabled
    Neural Network: ‚≠ï Disabled
  Advanced Models:
    XGBoost: ‚úÖ Enabled
    LightGBM: ‚úÖ Enabled
  Total Models Enabled: 2
  üí° Install XGBoost: pip install xgboost>=1.6.0
  üí° Install LightGBM: pip install synapseml>=0.11.0
Starting AutoML pipeline...
Target column: target
üìä Analyzing dataset size...
25/09/24 21:40:38 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:38 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
Training data shape: 100,000 rows, 300 columns
üìä Dataset size stored: large
üîç Detecting number of target classes...
25/09/24 21:40:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:40:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHNTaTVJV28yVU5RcRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:40:42 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHNTaTVJV28yVU5RcRoCamkaAmpm
‚úÖ Detected 2 classes. Multiclass: False

1. Data Preprocessing...
Starting data preprocessing...
üìä Using full dataset (sample_fraction = 1.0)
üîç Analyzing target column classes...
25/09/24 21:41:12 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:41:12 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGpHeXJ2dllNdldjVBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:12 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGpHeXJ2dllNdldjVBoCamkaAmpm
üìä Detected 2 classes using distinct count
Initial feature count: 297
üóìÔ∏è Automatically filtered out 1 date/timestamp columns:
   - SNAPSHOT_DAY_DESC (Spark date)
üí° Date columns are excluded because they often don't provide meaningful features for ML models
Features after date column filtering: 296
25/09/24 21:41:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:41:26 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:41:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,HOUSE_MDU_IND,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,CUSTOMER_FIBER_TENURE_MONTH,HAS_DATA_IND,HAS_VIDEO_IND,HAS_VOICE_IND,HAS_MOBILE_IND,FTTH_IND,BAC_IND,ACP_IND,OPTIMUM_COMPLETE_IND,PREMIUM_HBO_IND,PREMIUM_MAX_IND,STREAM_IND,FIXED_SUB_AUTO_PAY_IND,FIXED_CUSTOMER_EBILL_STATUS,PIA_PAID_IND,CARE_FIBER_OFFER_ELIGIBLE_IND,RETENTION_FIBER_OFFER_ELIGIBLE_IND,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,SPEED_ADDED_PREV_30D_IND,SPEED_ADDED_PREV_60D_IND,SPEED_ADDED_PREV_90D_IND,VIDEO_TIERS_ADDED_PREV_30D_IND,VIDEO_TIERS_ADDED_PREV_60D_IND,VIDEO_TIERS_ADDED_PREV_90D_IND,UPGRADE_IND_PREV_30D_IND,UPGRADE_IND_PREV_60D_IND,UPGRADE_IND_PREV_90D_IND,DATA_PSU_UPGRADES_PREV_30D_IND,DATA_PSU_UPGRADES_PREV_60D_IND,DATA_PSU_UPGRADES_PREV_90D_IND,VIDEO_PSU_UPGRADES_PREV_30D_IND,VIDEO_PSU_UPGRADES_PREV_60D_IND,VIDEO_PSU_UPGRADES_PREV_90D_IND,VOICE_PSU_UPGRADES_PREV_30D_IND,VOICE_PSU_UPGRADES_PREV_60D_IND,VOICE_PSU_UPGRADES_PREV_90D_IND,DOWNGRADE_IND_PREV_30D_IND,DOWNGRADE_IND_PREV_60D_IND,DOWNGRADE_IND_PREV_90D_IND,DATA_PSU_DOWNGRADES_PREV_30D_IND,DATA_PSU_DOWNGRADES_PREV_60D_IND,DATA_PSU_DOWNGRADES_PREV_90D_IND,VIDEO_PSU_DOWNGRADES_PREV_30D_IND,VIDEO_PSU_DOWNGRADES_PREV_60D_IND,VIDEO_PSU_DOWNGRADES_PREV_90D_IND,VOICE_PSU_DOWNGRADES_PREV_30D_IND,VOICE_PSU_DOWNGRADES_PREV_60D_IND,VOICE_PSU_DOWNGRADES_PREV_90D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_VOD_MINS_M0,USAGE_VIDEO_STB_DVR_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_VOICE_DOM_SECONDS_M0,USAGE_VOICE_DOM_CALLS_COUNT_M0,USAGE_SMS_COUNT_M0,USAGE_MMS_CNT_M0,USAGE_WIFI_INPUT_BYTES_M0,USAGE_WIFI_OUTPUT_BYTES_M0,USAGE_WIFI_DEVICES_COUNT_M0,USAGE_OPTIMUM_APP_VISIT_COUNT_M0,USAGE_OPTIMUM_APP_PAGE_COUNT_M0,USAGE_OPTIMUM_WEBSITE_VISIT_COUNT_M0,USAGE_OPTIMUM_WEBSITE_PAGE_COUNT_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,USAGE_VIDEO_STB_VOD_MINS_M1,USAGE_VIDEO_STB_DVR_MINS_M1,USAGE_WIFI_INPUT_BYTES_M1,USAGE_WIFI_OUTPUT_BYTES_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,FIXED_DATA_ROLLOFF_MONTH_LEFT,FIXED_VIDEO_ROLLOFF_MONTH_LEFT,FIXED_VOICE_ROLLOFF_MONTH_LEFT,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,BILLING_COLLECTION_STATUS_DESC,OVERDUE_BALANCE_30D,OVERDUE_BALANCE_60D,OVERDUE_BALANCE_90D,OVERDUE_BALANCE_120D,OVERDUE_BALANCE_OVR,AMT_PAY_IN_COLLECTIONS_PAST30D,AMT_PAY_IN_COLLECTIONS_PAST60D,AMT_PAY_IN_COLLECTIONS_PAST90D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST30D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST60D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST90D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST30D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST60D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST90D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D,CNT_PAY_IN_COLLECTIONS_PAST30D,CNT_PAY_IN_COLLECTIONS_PAST60D,CNT_PAY_IN_COLLECTIONS_PAST90D,DAYS_COLLECTIONS_PENDING_NPD_PAST30D,DAYS_COLLECTIONS_PENDING_NPD_PAST60D,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,DAYS_IN_COLLECTIONS_PAST30D,DAYS_IN_COLLECTIONS_PAST60D,DAYS_IN_COLLECTIONS_PAST90D,WO_RATE_EVENT_IND_PREV_60D_CNT,WO_RATE_EVENT_IND_PREV_180D_CNT,WO_PROMO_ROLLOFF_IND_PREV_60D_CNT,WO_PROMO_ROLLOFF_IND_PREV_180D_CNT,RATE_TOTAL_LIFT_AMT_M0,RATE_DATA_LIFT_AMT_M0,RATE_VIDEO_LIFT_AMT_M0,RATE_VOICE_LIFT_AMT_M0,RATE_TOTAL_LIFT_AMT_M1,RATE_DATA_LIFT_AMT_M1,RATE_VIDEO_LIFT_AMT_M1,RATE_VOICE_LIFT_AMT_M1,RATE_TOTAL_LIFT_AMT_M2,RATE_DATA_LIFT_AMT_M2,RATE_VIDEO_LIFT_AMT_M2,RATE_VOICE_LIFT_AMT_M2,CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT,IVR_FIXED_TOTAL_CNT_PREV_7D_CNT,IVR_CSR_FIXED_CNT_PREV_7D_CNT,IVR_TSR_FIXED_CNT_PREV_7D_CNT,IVR_SALES_FIXED_CNT_PREV_7D_CNT,IVR_RETENTION_FIXED_CNT_PREV_7D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_CSR_FIXED_CNT_PREV_30D_CNT,IVR_TSR_FIXED_CNT_PREV_30D_CNT,IVR_SALES_FIXED_CNT_PREV_30D_CNT,IVR_RETENTION_FIXED_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_60D_CNT,IVR_TSR_FIXED_CNT_PREV_60D_CNT,IVR_SALES_FIXED_CNT_PREV_60D_CNT,IVR_RETENTION_FIXED_CNT_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_90D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,IVR_SALES_FIXED_CNT_PREV_90D_CNT,IVR_RETENTION_FIXED_CNT_PREV_90D_CNT,TRUCK_ROLL_TOTAL_PREV_7D_CNT,TRUCK_ROLL_TOTAL_PREV_30D_CNT,TRUCK_ROLL_TOTAL_PREV_60D_CNT,TRUCK_ROLL_TOTAL_PREV_90D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_7D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_30D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_60D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_90D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_7D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_30D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_60D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_90D_CNT,HOUSE_COAX_COMP,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_UPLOAD_SPEED_LOWEST,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_VENDOR_CNT,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,FIXED_CONTACT_CENTER_NPS_RESPONSE_VALUE,FIXED_FIELD_OPS_NPS_RESPONSE_VALUE,FIXED_IVR_NPS_RESPONSE_VALUE,FIXED_OPTIMUM_ADVANCED_SUPPORT_NPS_RESPONSE_VALUE,FIXED_PTS_NPS_RESPONSE_VALUE,FIXED_RETENTION_NPS_RESPONSE_VALUE,FIXED_SALES_NPS_RESPONSE_VALUE,FIXED_SELF_INSTALL_NPS_RESPONSE_VALUE,FIXED_STORE_NPS_RESPONSE_VALUE],|filters=[]
25/09/24 21:41:27 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDMxZVhhT3lHd1dGMxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:27 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDMxZVhhT3lHd1dGMxoCamkaAmpm
Features after missing value filtering: 213
['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']
Categorical variables: 34
Numerical variables: 179
üîç Analyzing cardinality for 34 categorical variables...
üìä Computing cardinality and missing values for all 34 variables in batch operations...
25/09/24 21:41:38 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:41:39 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHc2bXVRM3ZwT2o0ZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:39 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHc2bXVRM3ZwT2o0ZBoCamkaAmpm
25/09/24 21:41:42 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB
25/09/24 21:42:05 WARN DAGScheduler: Broadcasting large task binary with size 24.4 MiB
25/09/24 21:42:16 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:42:17 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHllTFBmVmdZOF8wMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:17 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHllTFBmVmdZOF8wMRoCamkaAmpm
25/09/24 21:42:27 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:42:27 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
   üìä OMS_DIVISION: ~1 unique values ‚Üí Keeping as CATEGORICAL
   üìä OMS_AREA: ~3 unique values ‚Üí Keeping as CATEGORICAL
   üìä OMS_REGION: ~5 unique values ‚Üí Keeping as CATEGORICAL
   üìä OMS_MARKET: ~46 unique values ‚Üí Keeping as CATEGORICAL
   üìä HOUSE_CLEANSED_STATE: ~5 unique values ‚Üí Keeping as CATEGORICAL
   üìä HEAD_END_DESC: ~81 unique values ‚Üí DROPPING (exceeds threshold of 50)
   üìä DWELL_CODE: ~36 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_AGE_RANGE: ~5 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_ECOHORT_GROUP_DESC: ~17 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_SPENDING_TYPE_DESC: ~6 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_CREDIT_LINE_TYPE_DESC: ~6 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_CREDIT_USAGE_TYPE_DESC: ~6 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_LIFESTAGE_DESC: ~49 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_ARCHETYPE_DESC: ~9 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_ACCOUNT_STATUS: ~2 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_ACTIVE_RESI_BUS_DESC: ~1 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_CUSTOMER_EBILL_STATUS: ~3 unique values ‚Üí Keeping as CATEGORICAL
   üìä BOX_EQUIP_CLASS: ~4 unique values ‚Üí Keeping as CATEGORICAL
   üìä BOX_EQUIP_CLASS_GROUP: ~3 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_DATA_PRICEPOINT_TIER_DESC: ~27 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_VIDEO_TIER_DESC: ~18 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_VOICE_PRICEPOINT_TIER_DESC: ~14 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0: ~27 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_VIDEO_OFFER_DESC_M0: ~47 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0: ~9 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1: ~29 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_VIDEO_OFFER_DESC_M1: ~49 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1: ~9 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2: ~29 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_VIDEO_OFFER_DESC_M2: ~49 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2: ~9 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_COLLECTION_STATUS_DESC: ~2 unique values ‚Üí Keeping as CATEGORICAL
   üìä HOUSE_COPPER_COMP: ~3 unique values ‚Üí Keeping as CATEGORICAL
   üìä HOUSE_FIBER_COMP: ~4 unique values ‚Üí Keeping as CATEGORICAL
üóëÔ∏è Dropping 1 high-cardinality categorical variables
‚úÖ Keeping 33 low-cardinality categorical variables
üóëÔ∏è Removing 1 high-cardinality categorical variables...
üí° These variables have >50 unique values and would create too many features after encoding
   Dropping columns: HEAD_END_DESC
‚úÖ High-cardinality categorical variables removed from dataset
üìä This prevents curse of dimensionality and improves model performance
Updated categorical variables: 33
Numerical variables unchanged: 179
üîÑ Converting string columns to numeric where needed...
‚ôªÔ∏è Reusing cardinality stats for 33 categorical features...
‚úÖ Using pre-calculated cardinality statistics (threshold: 200 unique values)
   ‚ôªÔ∏è OMS_DIVISION: 1 unique values (reused)
   ‚ôªÔ∏è OMS_AREA: 3 unique values (reused)
   ‚ôªÔ∏è OMS_REGION: 5 unique values (reused)
   ‚ôªÔ∏è OMS_MARKET: 46 unique values (reused)
   ‚ôªÔ∏è HOUSE_CLEANSED_STATE: 5 unique values (reused)
   ‚ôªÔ∏è DWELL_CODE: 36 unique values (reused)
   ‚ôªÔ∏è DEMOS_AGE_RANGE: 5 unique values (reused)
   ‚ôªÔ∏è DEMOS_ECOHORT_GROUP_DESC: 17 unique values (reused)
   ‚ôªÔ∏è DEMOS_SPENDING_TYPE_DESC: 6 unique values (reused)
   ‚ôªÔ∏è DEMOS_CREDIT_LINE_TYPE_DESC: 6 unique values (reused)
   ‚ôªÔ∏è DEMOS_CREDIT_USAGE_TYPE_DESC: 6 unique values (reused)
   ‚ôªÔ∏è DEMOS_LIFESTAGE_DESC: 49 unique values (reused)
   ‚ôªÔ∏è DEMOS_ARCHETYPE_DESC: 9 unique values (reused)
   ‚ôªÔ∏è FIXED_ACCOUNT_STATUS: 2 unique values (reused)
   ‚ôªÔ∏è FIXED_ACTIVE_RESI_BUS_DESC: 1 unique values (reused)
   ‚ôªÔ∏è FIXED_CUSTOMER_EBILL_STATUS: 3 unique values (reused)
   ‚ôªÔ∏è BOX_EQUIP_CLASS: 4 unique values (reused)
   ‚ôªÔ∏è BOX_EQUIP_CLASS_GROUP: 3 unique values (reused)
   ‚ôªÔ∏è FIXED_DATA_PRICEPOINT_TIER_DESC: 27 unique values (reused)
   ‚ôªÔ∏è FIXED_VIDEO_TIER_DESC: 18 unique values (reused)
   ‚ôªÔ∏è FIXED_VOICE_PRICEPOINT_TIER_DESC: 14 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0: 27 unique values (reused)
   ‚ôªÔ∏è BILLING_VIDEO_OFFER_DESC_M0: 47 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0: 9 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1: 29 unique values (reused)
   ‚ôªÔ∏è BILLING_VIDEO_OFFER_DESC_M1: 49 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1: 9 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2: 29 unique values (reused)
   ‚ôªÔ∏è BILLING_VIDEO_OFFER_DESC_M2: 49 unique values (reused)
   ‚ôªÔ∏è BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2: 9 unique values (reused)
   ‚ôªÔ∏è BILLING_COLLECTION_STATUS_DESC: 2 unique values (reused)
   ‚ôªÔ∏è HOUSE_COPPER_COMP: 3 unique values (reused)
   ‚ôªÔ∏è HOUSE_FIBER_COMP: 4 unique values (reused)
üìà Categorical feature filtering: 33 ‚Üí 33 features (optimized)
25/09/24 21:42:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION],|filters=[]
25/09/24 21:42:29 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDER2NkxTMzVnT2dYMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:29 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDER2NkxTMzVnT2dYMhoCamkaAmpm
25/09/24 21:42:33 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_AREA],|filters=[]
25/09/24 21:42:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDENkMWtpSXdEZHNPVRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDENkMWtpSXdEZHNPVRoCamkaAmpm
25/09/24 21:42:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_REGION],|filters=[]
25/09/24 21:42:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFk5Rl9ZeXJuQlQyLRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFk5Rl9ZeXJuQlQyLRoCamkaAmpm
25/09/24 21:42:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_MARKET],|filters=[]
25/09/24 21:42:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDE0ySG9QQnBXYmF6OBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:43 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDE0ySG9QQnBXYmF6OBoCamkaAmpm
25/09/24 21:42:51 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_CLEANSED_STATE],|filters=[]
25/09/24 21:42:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGR0MzBNay12bG1oSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGR0MzBNay12bG1oSxoCamkaAmpm
25/09/24 21:42:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE],|filters=[]
25/09/24 21:42:56 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDC0zcHRZRkx0ZXFPNBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:56 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDC0zcHRZRkx0ZXFPNBoCamkaAmpm
25/09/24 21:42:59 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_AGE_RANGE],|filters=[]
25/09/24 21:43:00 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJOWUZsS1lFOGpFRBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:00 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJOWUZsS1lFOGpFRBoCamkaAmpm
25/09/24 21:43:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_ECOHORT_GROUP_DESC],|filters=[]
25/09/24 21:43:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFlxdWd0RDRRaEdKdhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFlxdWd0RDRRaEdKdhoCamkaAmpm
25/09/24 21:43:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_SPENDING_TYPE_DESC],|filters=[]
25/09/24 21:43:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGFhcS1ZWUdxMUZQNhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGFhcS1ZWUdxMUZQNhoCamkaAmpm
25/09/24 21:43:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_CREDIT_LINE_TYPE_DESC],|filters=[]
25/09/24 21:43:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDYwd2ZTSkxNQXNmMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDYwd2ZTSkxNQXNmMRoCamkaAmpm
25/09/24 21:43:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_CREDIT_USAGE_TYPE_DESC],|filters=[]
25/09/24 21:43:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGFrRU90X3JrZ0tvbBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGFrRU90X3JrZ0tvbBoCamkaAmpm
25/09/24 21:43:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC],|filters=[]
25/09/24 21:43:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJfUVp3TjMyck40MxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:08 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJfUVp3TjMyck40MxoCamkaAmpm
25/09/24 21:43:08 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_ARCHETYPE_DESC],|filters=[]
25/09/24 21:43:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGJrbDdTSGpsRjI3cRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:08 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGJrbDdTSGpsRjI3cRoCamkaAmpm
25/09/24 21:43:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_ACCOUNT_STATUS],|filters=[]
25/09/24 21:43:09 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGRIRmxYcVFDenhsRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:09 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGRIRmxYcVFDenhsRxoCamkaAmpm
25/09/24 21:43:13 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_ACTIVE_RESI_BUS_DESC],|filters=[]
25/09/24 21:43:13 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHZaS2NzMFJZV1NZVRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:13 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHZaS2NzMFJZV1NZVRoCamkaAmpm
25/09/24 21:43:13 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_CUSTOMER_EBILL_STATUS],|filters=[]
25/09/24 21:43:14 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFNsRnlTdi1IeENkTBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:14 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFNsRnlTdi1IeENkTBoCamkaAmpm
25/09/24 21:43:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:43:15 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFlGdG1nMU1BMHoyZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:15 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFlGdG1nMU1BMHoyZBoCamkaAmpm
25/09/24 21:43:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS_GROUP],|filters=[]
25/09/24 21:43:16 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGswdW0zZnlUU0EwXxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:16 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGswdW0zZnlUU0EwXxoCamkaAmpm
25/09/24 21:43:24 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_DATA_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:43:24 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGllUl82bXpHbDdOaBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:24 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGllUl82bXpHbDdOaBoCamkaAmpm
25/09/24 21:43:25 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_VIDEO_TIER_DESC],|filters=[]
25/09/24 21:43:25 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJQVlhmSlMtQm9KeBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:25 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJQVlhmSlMtQm9KeBoCamkaAmpm
25/09/24 21:43:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_VOICE_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:43:26 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGQ2SmlFNkVONVYwMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:26 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGQ2SmlFNkVONVYwMRoCamkaAmpm
25/09/24 21:43:27 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:43:27 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGx0U3hRV2hRZ0JNURoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:27 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGx0U3hRV2hRZ0JNURoCamkaAmpm
25/09/24 21:43:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M0],|filters=[]
25/09/24 21:43:28 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGVLbHVjVVdLNC02UhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:28 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGVLbHVjVVdLNC02UhoCamkaAmpm
25/09/24 21:43:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:43:29 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZUS2l0S0NnT3AxMBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:29 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDZUS2l0S0NnT3AxMBoCamkaAmpm
25/09/24 21:43:29 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1],|filters=[]
25/09/24 21:43:30 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFdTMGtKcjV0Y2pXcRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:30 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFdTMGtKcjV0Y2pXcRoCamkaAmpm
25/09/24 21:43:30 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M1],|filters=[]
25/09/24 21:43:30 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG10QjhRYTJwZFJDThoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:30 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDG10QjhRYTJwZFJDThoCamkaAmpm
25/09/24 21:43:31 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1],|filters=[]
25/09/24 21:43:31 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGhGTkxIbFBfMmcybBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:31 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGhGTkxIbFBfMmcybBoCamkaAmpm
25/09/24 21:43:32 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2],|filters=[]
25/09/24 21:43:32 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHBpQ1hraFVsVTVPRBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:32 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHBpQ1hraFVsVTVPRBoCamkaAmpm
25/09/24 21:43:33 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M2],|filters=[]
25/09/24 21:43:33 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGpkQnpDdVN6SEpnSRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:33 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGpkQnpDdVN6SEpnSRoCamkaAmpm
25/09/24 21:43:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2],|filters=[]
25/09/24 21:43:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGxBWUIxR3FHeURLNRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGxBWUIxR3FHeURLNRoCamkaAmpm
25/09/24 21:43:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_COLLECTION_STATUS_DESC],|filters=[]
25/09/24 21:43:35 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDBzSFFEdnJqRVJwLRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:35 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDBzSFFEdnJqRVJwLRoCamkaAmpm
25/09/24 21:43:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_COPPER_COMP],|filters=[]
25/09/24 21:43:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG5ud3pkMlVaekdDYRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDG5ud3pkMlVaekdDYRoCamkaAmpm
25/09/24 21:43:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:43:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEI1UmRwWjZteVhTSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEI1UmRwWjZteVhTSxoCamkaAmpm
Categorical encoding step complete.
Numerical impuation step complete.
Categorical encoding complete with _encoded suffix.
Target and Preprocessed features joined.
Final Data Summary - 
‚ö†Ô∏è Spark session recovery utilities not available, using basic error handling
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üìã Cached schema with 213 columns for recovery
ED_HIGHEST': 'bigint', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST': 'bigint', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG': 'double', 'OMS_DIVISION_encoded': 'double', 'OMS_AREA_encoded': 'double', 'OMS_REGION_encoded': 'double', 'OMS_MARKET_encoded': 'double', 'HOUSE_CLEANSED_STATE_encoded': 'double', 'DWELL_CODE_encoded': 'double', 'DEMOS_AGE_RANGE_encoded': 'double', 'DEMOS_ECOHORT_GROUP_DESC_encoded': 'double', 'DEMOS_SPENDING_TYPE_DESC_encoded': 'double', 'DEMOS_CREDIT_LINE_TYPE_DESC_encoded': 'double', 'DEMOS_CREDIT_USAGE_TYPE_DESC_encoded': 'double', 'DEMOS_LIFESTAGE_DESC_encoded': 'double', 'DEMOS_ARCHETYPE_DESC_encoded': 'double', 'FIXED_ACCOUNT_STATUS_encoded': 'double', 'FIXED_ACTIVE_RESI_BUS_DESC_encoded': 'double', 'FIXED_CUSTOMER_EBILL_STATUS_encoded': 'double', 'BOX_EQUIP_CLASS_encoded': 'double', 'BOX_EQUIP_CLASS_GROUP_encoded': 'double', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded': 'double', 'FIXED_VIDEO_TIER_DESC_encoded': 'double', 'FIXED_VOICE_PRICEPOINT_TIER_DESC_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M0_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M1_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M2_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2_encoded': 'double', 'BILLING_COLLECTION_STATUS_DESC_encoded': 'double', 'HOUSE_COPPER_COMP_encoded': 'double', 'HOUSE_FIBER_COMP_encoded': 'double', 'target': 'bigint'}}
Data preprocessing completed.
Feature variables: ['SNAPSHOT_DAY_DESC', 'OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'CUSTOMER_FIBER_TENURE_MONTH', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_VOICE_DOM_SECONDS_M0', 'USAGE_VOICE_DOM_CALLS_COUNT_M0', 'USAGE_SMS_COUNT_M0', 'USAGE_MMS_CNT_M0', 'USAGE_WIFI_INPUT_BYTES_M0', 'USAGE_WIFI_OUTPUT_BYTES_M0', 'USAGE_WIFI_DEVICES_COUNT_M0', 'USAGE_OPTIMUM_APP_VISIT_COUNT_M0', 'USAGE_OPTIMUM_APP_PAGE_COUNT_M0', 'USAGE_OPTIMUM_WEBSITE_VISIT_COUNT_M0', 'USAGE_OPTIMUM_WEBSITE_PAGE_COUNT_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'USAGE_WIFI_INPUT_BYTES_M1', 'USAGE_WIFI_OUTPUT_BYTES_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'FIXED_DATA_ROLLOFF_MONTH_LEFT', 'FIXED_VIDEO_ROLLOFF_MONTH_LEFT', 'FIXED_VOICE_ROLLOFF_MONTH_LEFT', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'AMT_PAY_IN_COLLECTIONS_PAST30D', 'AMT_PAY_IN_COLLECTIONS_PAST60D', 'AMT_PAY_IN_COLLECTIONS_PAST90D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST30D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST60D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST90D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST30D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST60D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'CNT_PAY_IN_COLLECTIONS_PAST30D', 'CNT_PAY_IN_COLLECTIONS_PAST60D', 'CNT_PAY_IN_COLLECTIONS_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'RATE_TOTAL_LIFT_AMT_M0', 'RATE_DATA_LIFT_AMT_M0', 'RATE_VIDEO_LIFT_AMT_M0', 'RATE_VOICE_LIFT_AMT_M0', 'RATE_TOTAL_LIFT_AMT_M1', 'RATE_DATA_LIFT_AMT_M1', 'RATE_VIDEO_LIFT_AMT_M1', 'RATE_VOICE_LIFT_AMT_M1', 'RATE_TOTAL_LIFT_AMT_M2', 'RATE_DATA_LIFT_AMT_M2', 'RATE_VIDEO_LIFT_AMT_M2', 'RATE_VOICE_LIFT_AMT_M2', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_90D_CNT', 'HOUSE_COAX_COMP', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'FIXED_CONTACT_CENTER_NPS_RESPONSE_VALUE', 'FIXED_FIELD_OPS_NPS_RESPONSE_VALUE', 'FIXED_IVR_NPS_RESPONSE_VALUE', 'FIXED_OPTIMUM_ADVANCED_SUPPORT_NPS_RESPONSE_VALUE', 'FIXED_PTS_NPS_RESPONSE_VALUE', 'FIXED_RETENTION_NPS_RESPONSE_VALUE', 'FIXED_SALES_NPS_RESPONSE_VALUE', 'FIXED_SELF_INSTALL_NPS_RESPONSE_VALUE', 'FIXED_STORE_NPS_RESPONSE_VALUE']
Selected variables: ['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']
Categorical variables: ['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'DWELL_CODE', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_CUSTOMER_EBILL_STATUS', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'BILLING_COLLECTION_STATUS_DESC', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP']
Numerical variables: ['HOUSE_MDU_IND', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']

2. Feature Selection...
üìä Available features: 212, Configured max: 50
üéØ Will select: 50 features
Feature selection: selecting top 50 features...
üìä Total features available: 212
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üìä Dataset size: 100,000 rows
üìä Dataset category: large
üìä Using full dataset (100,000 rows) for feature selection
üîÑ Large feature set detected (212 > 200). Using sequential feature selection...
Processing 212 features in chunks of 100
Created 3 chunks

Processing chunk 1/3 with 100 features...

üéØ Running Feature Importance for Classification
   üìä Features available: 100
   üéØ Features to select: 30
   üìÇ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   üë§ User ID: automl_user
   üè∑Ô∏è Model literal: automl_model
   üéØ Target column: target
üìä Will rank all 100 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üß† Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:43:42 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_MDU_IND,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,HAS_DATA_IND,HAS_VIDEO_IND,HAS_VOICE_IND,HAS_MOBILE_IND,FTTH_IND,BAC_IND,ACP_IND,OPTIMUM_COMPLETE_IND,PREMIUM_HBO_IND,PREMIUM_MAX_IND,STREAM_IND,FIXED_SUB_AUTO_PAY_IND,PIA_PAID_IND,CARE_FIBER_OFFER_ELIGIBLE_IND,RETENTION_FIBER_OFFER_ELIGIBLE_IND,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,SPEED_ADDED_PREV_30D_IND,SPEED_ADDED_PREV_60D_IND,SPEED_ADDED_PREV_90D_IND,VIDEO_TIERS_ADDED_PREV_30D_IND,VIDEO_TIERS_ADDED_PREV_60D_IND,VIDEO_TIERS_ADDED_PREV_90D_IND,UPGRADE_IND_PREV_30D_IND,UPGRADE_IND_PREV_60D_IND,UPGRADE_IND_PREV_90D_IND,DATA_PSU_UPGRADES_PREV_30D_IND,DATA_PSU_UPGRADES_PREV_60D_IND,DATA_PSU_UPGRADES_PREV_90D_IND,VIDEO_PSU_UPGRADES_PREV_30D_IND,VIDEO_PSU_UPGRADES_PREV_60D_IND,VIDEO_PSU_UPGRADES_PREV_90D_IND,VOICE_PSU_UPGRADES_PREV_30D_IND,VOICE_PSU_UPGRADES_PREV_60D_IND,VOICE_PSU_UPGRADES_PREV_90D_IND,DOWNGRADE_IND_PREV_30D_IND,DOWNGRADE_IND_PREV_60D_IND,DOWNGRADE_IND_PREV_90D_IND,DATA_PSU_DOWNGRADES_PREV_30D_IND,DATA_PSU_DOWNGRADES_PREV_60D_IND,DATA_PSU_DOWNGRADES_PREV_90D_IND,VIDEO_PSU_DOWNGRADES_PREV_30D_IND,VIDEO_PSU_DOWNGRADES_PREV_60D_IND,VIDEO_PSU_DOWNGRADES_PREV_90D_IND,VOICE_PSU_DOWNGRADES_PREV_30D_IND,VOICE_PSU_DOWNGRADES_PREV_60D_IND,VOICE_PSU_DOWNGRADES_PREV_90D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_VOD_MINS_M0,USAGE_VIDEO_STB_DVR_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,USAGE_VIDEO_STB_VOD_MINS_M1,USAGE_VIDEO_STB_DVR_MINS_M1,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_30D,OVERDUE_BALANCE_60D,OVERDUE_BALANCE_90D,OVERDUE_BALANCE_120D,OVERDUE_BALANCE_OVR,CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D,DAYS_COLLECTIONS_PENDING_NPD_PAST30D,DAYS_COLLECTIONS_PENDING_NPD_PAST60D,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,DAYS_IN_COLLECTIONS_PAST30D,DAYS_IN_COLLECTIONS_PAST60D,DAYS_IN_COLLECTIONS_PAST90D,WO_RATE_EVENT_IND_PREV_60D_CNT,WO_RATE_EVENT_IND_PREV_180D_CNT,WO_PROMO_ROLLOFF_IND_PREV_60D_CNT,WO_PROMO_ROLLOFF_IND_PREV_180D_CNT,CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT],|filters=[]
25/09/24 21:43:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHI0THV3LVRoMGJIMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:42 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHI0THV3LVRoMGJIMhoCamkaAmpm
25/09/24 21:43:42 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:43:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEhUbmxrbXN2S2MzMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:43 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEhUbmxrbXN2S2MzMhoCamkaAmpm
‚úÖ Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
‚úÖ Successfully selected 30 features using Random Forest
Selected 30 features from chunk 1
Cleaned up chunk data, pausing briefly...

Processing chunk 2/3 with 100 features...

üéØ Running Feature Importance for Classification
   üìä Features available: 100
   üéØ Features to select: 30
   üìÇ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   üë§ User ID: automl_user
   üè∑Ô∏è Model literal: automl_model
   üéØ Target column: target
üìä Will rank all 100 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:02 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:02 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:02 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:02 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:03 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:03 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üß† Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:03 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT,IVR_FIXED_TOTAL_CNT_PREV_7D_CNT,IVR_CSR_FIXED_CNT_PREV_7D_CNT,IVR_TSR_FIXED_CNT_PREV_7D_CNT,IVR_SALES_FIXED_CNT_PREV_7D_CNT,IVR_RETENTION_FIXED_CNT_PREV_7D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_CSR_FIXED_CNT_PREV_30D_CNT,IVR_TSR_FIXED_CNT_PREV_30D_CNT,IVR_SALES_FIXED_CNT_PREV_30D_CNT,IVR_RETENTION_FIXED_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_60D_CNT,IVR_TSR_FIXED_CNT_PREV_60D_CNT,IVR_SALES_FIXED_CNT_PREV_60D_CNT,IVR_RETENTION_FIXED_CNT_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_90D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,IVR_SALES_FIXED_CNT_PREV_90D_CNT,IVR_RETENTION_FIXED_CNT_PREV_90D_CNT,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_UPLOAD_SPEED_LOWEST,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_VENDOR_CNT,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_DIVISION],|filters=[]
25/09/24 21:44:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHZnYTY0NnhfbGYzahoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHZnYTY0NnhfbGYzahoCamkaAmpm
25/09/24 21:44:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHhEZ1A4TTFNTU5pahoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHhEZ1A4TTFNTU5pahoCamkaAmpm
‚úÖ Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
‚úÖ Successfully selected 30 features using Random Forest
Selected 30 features from chunk 2
Cleaned up chunk data, pausing briefly...

Processing chunk 3/3 with 12 features...

üéØ Running Feature Importance for Classification
   üìä Features available: 12
   üéØ Features to select: 30
   üìÇ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   üë§ User ID: automl_user
   üè∑Ô∏è Model literal: automl_model
   üéØ Target column: target
üìä Will rank all 12 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:22 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üß† Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:44:22 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFZpNEFwRVZuM3BLRRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:22 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFZpNEFwRVZuM3BLRRoCamkaAmpm
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:23 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFDMTZiX3NpbmQycBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:23 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFDMTZiX3NpbmQycBoCamkaAmpm
‚úÖ Cached 100,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
‚úÖ Successfully selected 12 features using Random Forest
Selected 12 features from chunk 3

Combined 72 features from all chunks
Running final feature selection to select top 50 from 72 features...
Processing 72 features in chunks of 100
Created 1 chunks

Processing chunk 1/1 with 72 features...

üéØ Running Feature Importance for Classification
   üìä Features available: 72
   üéØ Features to select: 30
   üìÇ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   üë§ User ID: automl_user
   üè∑Ô∏è Model literal: automl_model
   üéØ Target column: target
üìä Will rank all 72 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
üß† Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,FIXED_SUB_AUTO_PAY_IND,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,SPEED_ADDED_PREV_30D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,BILLING_COLLECTION_STATUS_DESC,OVERDUE_BALANCE_OVR,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFZc0NVUzhDNHJvOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFZc0NVUzhDNHJvOBoCamkaAmpm
25/09/24 21:44:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:37 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDdzcUdHOHVxYk42cBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:37 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDdzcUdHOHVxYk42cBoCamkaAmpm
‚úÖ Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
‚úÖ Successfully selected 30 features using Random Forest
Selected 30 features from chunk 1

Combined 30 features from all chunks
Combined features (30) <= max_features (50), using all
‚úÖ Feature selection completed!

üìä Generating final feature importance plot for 30 selected features...
25/09/24 21:44:48 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZLUzhJZmpWdUxvVBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDZLUzhJZmpWdUxvVBoCamkaAmpm
25/09/24 21:44:49 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHFEYWRmQ3g1TkZubxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHFEYWRmQ3g1TkZubxoCamkaAmpm
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDNUNUxQanlwaGhTdxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:50 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDNUNUxQanlwaGhTdxoCamkaAmpm
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:50 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEw1eHBSMHZrTGpSMBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEw1eHBSMHZrTGpSMBoCamkaAmpm
25/09/24 21:44:51 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFc2UUptbElnd2JGcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFc2UUptbElnd2JGcxoCamkaAmpm
25/09/24 21:44:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGgtaXlUcVMtVlZLcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGgtaXlUcVMtVlZLcxoCamkaAmpm
25/09/24 21:44:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHBrbWdmQW1lZEJpcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHBrbWdmQW1lZEJpcxoCamkaAmpm
25/09/24 21:44:53 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:53 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFBNFNoQXhoYzJ5NhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:53 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFBNFNoQXhoYzJ5NhoCamkaAmpm
25/09/24 21:45:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDE2LTl5ODhWMVhtZRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDE2LTl5ODhWMVhtZRoCamkaAmpm
25/09/24 21:45:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHRCcGR3UTJ5WVQ2QxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHRCcGR3UTJ5WVQ2QxoCamkaAmpm
25/09/24 21:45:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHRvU2M5dFpiZS1LSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHRvU2M5dFpiZS1LSxoCamkaAmpm
25/09/24 21:45:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGl6TVVyb0xrRVNmUBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGl6TVVyb0xrRVNmUBoCamkaAmpm
25/09/24 21:45:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFVIRTV0VE5hZEJ1ORoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFVIRTV0VE5hZEJ1ORoCamkaAmpm
25/09/24 21:45:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDF9COW55SV80UGp1dRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDF9COW55SV80UGp1dRoCamkaAmpm
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
‚úÖ Feature importance saved to Excel: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx
‚úÖ Enhanced feature importance plot saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/Features_selected_for_modeling.png
‚úÖ Feature importance Excel saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx
‚úÖ Final feature importance saved:
   üìä Excel file: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx (exists: True)
   üìà Plot file: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/Features_selected_for_modeling.png (exists: True)
Selected 30 features out of 212

üîç Checking if full dataset switch is needed...
   Has _original_table_reference: True
   Has _data_manager: True

2.5. üîÑ Switching to Full Dataset for Model Training...
25/09/24 21:45:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:09 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:45:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:09 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:45:10 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:10 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
   üìä Current dataset size: 100,000 rows
   üéØ Selected features: 30 features
   üîÑ Loading full dataset with selected features...
üîÑ Switching to full dataset for model training...
üìä Creating optimized table with 30 selected features
üîÑ Mapped 30 encoded features to raw names:
   Example: OMS_MARKET_encoded ‚Üí OMS_MARKET
üìä Creating optimized temp table from original atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
üîç No filters specified - using full table
üìã Selecting 31 columns: ['target', 'OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0']...
üîß Creating optimized temp table: atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852
‚úÖ Optimized temp table created: atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852 (expires in 24h)
25/09/24 21:45:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:14 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:14 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
‚úÖ Full dataset loaded: 2,596,745 rows √ó 31 columns
üéØ Optimized: Full rows with selected features only
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:15 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   üìä Full dataset loaded: 2,596,745 rows
   üìä Full dataset columns: ['target', 'OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'BILLING_VIDEO_OFFER_DESC_M2', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'BILLING_VIDEO_OFFER_DESC_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS']
   üßπ Clearing cached data before full dataset processing...
   üîÑ Reprocessing full dataset with same preprocessing...
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:15 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   üìä Full data before preprocessing: 2,596,745 rows
Starting data preprocessing...
üìä Using full dataset (sample_fraction = 1.0)
üîç Analyzing target column classes...
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:15 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEZzeDkzN1VadmZNNhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:15 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEZzeDkzN1VadmZNNhoCamkaAmpm
üìä Detected 2 classes using distinct count
Initial feature count: 30
Features after date column filtering: 30
25/09/24 21:45:18 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:18 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:18 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,BILLING_VIDEO_OFFER_DESC_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:18 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 31 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEFPajBicU8xaF9yUhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:18 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEFPajBicU8xaF9yUhoCamkaAmpm
Features after missing value filtering: 30
['OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'BILLING_VIDEO_OFFER_DESC_M2', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'BILLING_VIDEO_OFFER_DESC_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS']
Categorical variables: 10
Numerical variables: 20
üîç Analyzing cardinality for 10 categorical variables...
üìä Computing cardinality and missing values for all 10 variables in batch operations...
25/09/24 21:45:20 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,FIXED_VIDEO_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M0,DEMOS_ARCHETYPE_DESC,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:21 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 13 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDER2eElxTlItT09wRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:21 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDER2eElxTlItT09wRxoCamkaAmpm
25/09/24 21:45:21 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB
25/09/24 21:45:32 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB
25/09/24 21:45:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,FIXED_VIDEO_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M0,DEMOS_ARCHETYPE_DESC,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 13 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG9nZW5FOTJwMmRwchoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDG9nZW5FOTJwMmRwchoCamkaAmpm
25/09/24 21:45:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:43 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   üìä OMS_MARKET: ~47 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_LIFESTAGE_DESC: ~49 unique values ‚Üí Keeping as CATEGORICAL
   üìä DWELL_CODE: ~36 unique values ‚Üí Keeping as CATEGORICAL
   üìä DEMOS_ECOHORT_GROUP_DESC: ~17 unique values ‚Üí Keeping as CATEGORICAL
   üìä FIXED_VIDEO_TIER_DESC: ~25 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_VIDEO_OFFER_DESC_M2: ~108 unique values ‚Üí DROPPING (exceeds threshold of 50)
   üìä FIXED_DATA_PRICEPOINT_TIER_DESC: ~37 unique values ‚Üí Keeping as CATEGORICAL
   üìä BILLING_VIDEO_OFFER_DESC_M0: ~100 unique values ‚Üí DROPPING (exceeds threshold of 50)
   üìä DEMOS_ARCHETYPE_DESC: ~9 unique values ‚Üí Keeping as CATEGORICAL
   üìä BOX_EQUIP_CLASS: ~4 unique values ‚Üí Keeping as CATEGORICAL
üóëÔ∏è Dropping 2 high-cardinality categorical variables
‚úÖ Keeping 8 low-cardinality categorical variables
üóëÔ∏è Removing 2 high-cardinality categorical variables...
üí° These variables have >50 unique values and would create too many features after encoding
   Dropping columns: BILLING_VIDEO_OFFER_DESC_M2, BILLING_VIDEO_OFFER_DESC_M0
‚úÖ High-cardinality categorical variables removed from dataset
üìä This prevents curse of dimensionality and improves model performance
Updated categorical variables: 8
Numerical variables unchanged: 20
üîÑ Converting string columns to numeric where needed...
‚ôªÔ∏è Reusing cardinality stats for 8 categorical features...
‚úÖ Using pre-calculated cardinality statistics (threshold: 200 unique values)
   ‚ôªÔ∏è OMS_MARKET: 47 unique values (reused)
   ‚ôªÔ∏è DEMOS_LIFESTAGE_DESC: 49 unique values (reused)
   ‚ôªÔ∏è DWELL_CODE: 36 unique values (reused)
   ‚ôªÔ∏è DEMOS_ECOHORT_GROUP_DESC: 17 unique values (reused)
   ‚ôªÔ∏è FIXED_VIDEO_TIER_DESC: 25 unique values (reused)
   ‚ôªÔ∏è FIXED_DATA_PRICEPOINT_TIER_DESC: 37 unique values (reused)
   ‚ôªÔ∏è DEMOS_ARCHETYPE_DESC: 9 unique values (reused)
   ‚ôªÔ∏è BOX_EQUIP_CLASS: 4 unique values (reused)
üìà Categorical feature filtering: 8 ‚Üí 8 features (optimized)
25/09/24 21:45:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET],|filters=[]
25/09/24 21:45:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDIzdjgxeUR2V1QtThoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:44 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDDIzdjgxeUR2V1QtThoCamkaAmpm
25/09/24 21:45:44 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC],|filters=[]
25/09/24 21:45:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGkwV3RjcEVJRTdERBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:45 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGkwV3RjcEVJRTdERBoCamkaAmpm
25/09/24 21:45:46 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DWELL_CODE],|filters=[]
25/09/24 21:45:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGJKdVRVUEIxMkNrOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:46 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGJKdVRVUEIxMkNrOBoCamkaAmpm
25/09/24 21:45:47 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_ECOHORT_GROUP_DESC],|filters=[]
25/09/24 21:45:48 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 3 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGV3Tnpod25ybU1jbxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:48 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGV3Tnpod25ybU1jbxoCamkaAmpm
25/09/24 21:45:49 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[FIXED_VIDEO_TIER_DESC],|filters=[]
25/09/24 21:45:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFo5d053aEg3ZWIxbRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDFo5d053aEg3ZWIxbRoCamkaAmpm
25/09/24 21:45:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[FIXED_DATA_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:45:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZ6YlRpT2MwSmRyTRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:50 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDDZ6YlRpT2MwSmRyTRoCamkaAmpm
25/09/24 21:45:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_ARCHETYPE_DESC],|filters=[]
25/09/24 21:45:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHJOYjl0TDVOTW91RhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDHJOYjl0TDVOTW91RhoCamkaAmpm
25/09/24 21:45:53 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:54 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHlvMFlzV3R2c2JUSRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:54 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDHlvMFlzV3R2c2JUSRoCamkaAmpm
Categorical encoding step complete.
Numerical impuation step complete.
Categorical encoding complete with _encoded suffix.
Target and Preprocessed features joined.
Final Data Summary - 
‚ö†Ô∏è Spark session recovery utilities not available, using basic error handling
25/09/24 21:45:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:55 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:55 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:57 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:57 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
üìã Cached schema with 29 columns for recovery
{'row_count': 2596745, 'column_count': 29, 'columns': ['FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'TOTAL_RECURRING_REVENUE', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'DWELL_CODE_encoded', 'DEMOS_ECOHORT_GROUP_DESC_encoded', 'FIXED_VIDEO_TIER_DESC_encoded', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded', 'DEMOS_ARCHETYPE_DESC_encoded', 'BOX_EQUIP_CLASS_encoded', 'target'], 'dtypes': {'FIXED_TENURE_DAYS': 'bigint', 'USAGE_TOTAL_UP_BYTES_M0': 'decimal(38,9)', 'TOTAL_RECURRING_REVENUE': 'decimal(38,9)', 'FIXED_DATA_TENURE_DAYS': 'bigint', 'TOTAL_DATA_RECURRING_REVENUE': 'decimal(38,9)', 'USAGE_TOTAL_DOWN_BYTES_M1': 'decimal(38,9)', 'HOUSE_COMP_UPLOAD_SPEED_AVG': 'double', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT': 'decimal(38,9)', 'VIDEO_RECURRING_REVENUE': 'decimal(38,9)', 'USAGE_TOTAL_DOWN_BYTES_M0': 'decimal(38,9)', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG': 'double', 'USAGE_TOTAL_UP_BYTES_M1': 'decimal(38,9)', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG': 'double', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT': 'decimal(38,9)', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG': 'double', 'OVERDUE_BALANCE_OVR': 'decimal(38,9)', 'FIXED_DATA_UPLOAD_SPEED': 'decimal(38,9)', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1': 'bigint', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT': 'decimal(38,9)', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX': 'decimal(38,9)', 'OMS_MARKET_encoded': 'double', 'DEMOS_LIFESTAGE_DESC_encoded': 'double', 'DWELL_CODE_encoded': 'double', 'DEMOS_ECOHORT_GROUP_DESC_encoded': 'double', 'FIXED_VIDEO_TIER_DESC_encoded': 'double', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded': 'double', 'DEMOS_ARCHETYPE_DESC_encoded': 'double', 'BOX_EQUIP_CLASS_encoded': 'double', 'target': 'bigint'}}
Data preprocessing completed.
25/09/24 21:45:58 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:58 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:58 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:58 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:46:00 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:46:00 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   üìä Full data after preprocessing: 2,596,745 rows
   üíæ Caching full_processed_data to prevent lazy evaluation issues...
25/09/24 21:46:01 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS,OMS_MARKET],|filters=[]
25/09/24 21:46:01 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 27 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEVfRW1lT3BaU1FBOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:01 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEVfRW1lT3BaU1FBOBoCamkaAmpm
25/09/24 21:46:01 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:46:01 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGZkMHB1QWxxOXRWRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:01 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGZkMHB1QWxxOXRWRxoCamkaAmpm
   ‚úÖ Full processed data cached and materialized
   ‚ö†Ô∏è Feature BILLING_VIDEO_OFFER_DESC_M2_encoded not found in processed data (neither original nor encoded)
   ‚ö†Ô∏è Feature BILLING_VIDEO_OFFER_DESC_M0_encoded not found in processed data (neither original nor encoded)
   üîÑ Selecting 28 encoded features + target from processed data
   üìã Required columns: ['OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE_encoded']...
   üîç DEBUG: full_processed_data before select: 76,251 rows
   üîç DEBUG: Checking if all required columns exist in processed data...
   ‚úÖ All 29 required columns found in processed data
   üîç DEBUG: full_processed_data_final after select: 76,251 rows
   üîÑ Created full_processed_data_final with 29 columns
   üîÑ Switched processed_data from sampled to full dataset
   üîÑ Updated selected_vars to use encoded feature names: 28 features
   üíæ Cached final processed_data for efficient reuse
   üßπ Cleaning up intermediate full_processed_data cache...
25/09/24 21:46:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS,OMS_MARKET],|filters=[]
25/09/24 21:46:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 27 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEpZYV9jSzBxeDNBZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEpZYV9jSzBxeDNBZBoCamkaAmpm
25/09/24 21:46:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:46:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFB5NC1VelU2Z256LRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDFB5NC1VelU2Z256LRoCamkaAmpm
   ‚úÖ Successfully switched to full dataset: 76,251 rows
Final selected variables: ['OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE_encoded', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC_encoded', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC_encoded', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC_encoded', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS_encoded']
Final categorical variables: []
Final numerical variables: ['FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'OVERDUE_BALANCE_OVR', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']

3. Data Splitting and Scaling...
üîç VERIFICATION: processed_data contains 76,251 rows before split_and_scale
üîç VERIFICATION: data contains 76,251 rows before split_and_scale
Splitting data into train/validation/test sets...

üìä Analyzing class distribution on full dataset...
üîç VERIFICATION: data contains 76,251 rows before analyze_class_distribution
Analyzing class distribution for target...
üîç VERIFICATION: data contains 76,251 rows before analyze_class_distribution
üîç VERIFICATION: total_count contains 76,251 rows after analyze_class_distribution
üîç VERIFICATION: Sum of class counts: 76,251 rows
‚úÖ Class count verification passed: 76,251 = 76,251
Class distribution:
  Class 1: 771 samples (1.01%)
  Class 0: 75480 samples (98.99%)
‚ö†Ô∏è  Imbalanced dataset detected: 1 class(es) below 5.0% threshold
    - Class 1: 1.01% (< 5.0%)
üîç VERIFICATION: data contains 76,251 rows after analyze_class_distribution
üîç VERIFICATION: data contains 76,251 rows after train_valid_test_split
üîç VERIFICATION: train contains 45,779 rows after train_valid_test_split
üîç VERIFICATION: valid contains 15,029 rows after train_valid_test_split
üîç VERIFICATION: test contains 15,443 rows after train_valid_test_split

üìà Applying oversample to training data...
Upsampling minority classes using oversample method...
Target minority class size: 3774 samples (5.0% of majority class)
  Upsampling class 1: 771 ‚Üí 3774 (+3003 samples)
Upsampling completed. New dataset size: 47670 samples

üìä Post-upsampling class distribution:
Analyzing class distribution for target...
üîç VERIFICATION: data contains 47,670 rows before analyze_class_distribution
üîç VERIFICATION: total_count contains 47,670 rows after analyze_class_distribution
üîç VERIFICATION: Sum of class counts: 47,670 rows
‚úÖ Class count verification passed: 47,670 = 47,670
Class distribution:
  Class 1: 2366 samples (4.96%)
  Class 0: 45304 samples (95.04%)
‚úÖ Training data upsampling completed. Validation/test data remain original.
üìã Note: Model will train on upsampled data, but training evaluation will use original data.
Data splitting and scaling completed.
üîç VERIFICATION: Train_scaled contains 47,670 rows after split_and_scale
üîç VERIFICATION: Train_original_scaled contains 45,779 rows after split_and_scale
üîç VERIFICATION: valid_scaled contains 15,029 rows after split_and_scale
üîç VERIFICATION: test_scaled contains 15,443 rows after split_and_scale

4. Preparing Out-of-Time Datasets...

üöÄ Optimizing OOT datasets to use only selected features...
üìã Required columns for OOT datasets: 29 columns
   Features: 28, Target: 1

4.5. Feature Profiling for All Datasets...
‚ôªÔ∏è Reusing cardinality stats for 10 categorical features
üîç Profiling 1 datasets with 30 selected features

üìä Profiling train dataset...
‚ôªÔ∏è Using existing cardinality stats for train dataset
üîÑ Mapped 30 encoded features to 20 raw features for profiling
   Example: OMS_MARKET_encoded ‚Üí FIXED_TENURE_DAYS
üîç Starting feature profiling for 20 selected features...
üìä Performing combined univariate analysis and data quality assessment...
‚ôªÔ∏è Reusing cardinality and missing value statistics...
   üöÄ Performing vectorized analysis for 20 features...
   üìä Processing 20 numerical and 0 categorical features...
   üßÆ Computing all numerical statistics in single operation...
   üîç Performing vectorized quality assessment...
     üîç Calculating missing unique count for FIXED_TENURE_DAYS...
     ‚úÖ FIXED_TENURE_DAYS: 13558 unique values calculated
     üîç Calculating missing unique count for USAGE_TOTAL_UP_BYTES_M0...
     ‚úÖ USAGE_TOTAL_UP_BYTES_M0: 68498 unique values calculated
     üîç Calculating missing unique count for TOTAL_RECURRING_REVENUE...
     ‚úÖ TOTAL_RECURRING_REVENUE: 5696 unique values calculated
     üîç Calculating missing unique count for FIXED_DATA_TENURE_DAYS...
     ‚úÖ FIXED_DATA_TENURE_DAYS: 7692 unique values calculated
     üîç Calculating missing unique count for TOTAL_DATA_RECURRING_REVENUE...
     ‚úÖ TOTAL_DATA_RECURRING_REVENUE: 1593 unique values calculated
     üîç Calculating missing unique count for USAGE_TOTAL_DOWN_BYTES_M1...
     ‚úÖ USAGE_TOTAL_DOWN_BYTES_M1: 68238 unique values calculated
     üîç Calculating missing unique count for HOUSE_COMP_UPLOAD_SPEED_AVG...
     ‚úÖ HOUSE_COMP_UPLOAD_SPEED_AVG: 1587 unique values calculated
     üîç Calculating missing unique count for IVR_FIXED_TOTAL_CNT_PREV_30D_CNT...
     ‚úÖ IVR_FIXED_TOTAL_CNT_PREV_30D_CNT: 40 unique values calculated
     üîç Calculating missing unique count for VIDEO_RECURRING_REVENUE...
     ‚úÖ VIDEO_RECURRING_REVENUE: 1844 unique values calculated
     üîç Calculating missing unique count for USAGE_TOTAL_DOWN_BYTES_M0...
     ‚úÖ USAGE_TOTAL_DOWN_BYTES_M0: 68630 unique values calculated
     üîç Calculating missing unique count for HOUSE_COMP_DOWNLOAD_SPEED_AVG...
     ‚úÖ HOUSE_COMP_DOWNLOAD_SPEED_AVG: 1685 unique values calculated
     üîç Calculating missing unique count for USAGE_TOTAL_UP_BYTES_M1...
     ‚úÖ USAGE_TOTAL_UP_BYTES_M1: 68209 unique values calculated
     üîç Calculating missing unique count for HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG...
     ‚úÖ HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG: 232 unique values calculated
     üîç Calculating missing unique count for CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT...
     ‚úÖ CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT: 8 unique values calculated
     üîç Calculating missing unique count for HOUSE_COMP_FW_UPLOAD_SPEED_AVG...
     ‚úÖ HOUSE_COMP_FW_UPLOAD_SPEED_AVG: 270 unique values calculated
     üîç Calculating missing unique count for OVERDUE_BALANCE_OVR...
     ‚úÖ OVERDUE_BALANCE_OVR: 5407 unique values calculated
     üîç Calculating missing unique count for FIXED_DATA_UPLOAD_SPEED...
     ‚úÖ FIXED_DATA_UPLOAD_SPEED: 17 unique values calculated
     üîç Calculating missing unique count for USAGE_VIDEO_STB_NO_VOD_MINS_M1...
     ‚úÖ USAGE_VIDEO_STB_NO_VOD_MINS_M1: 20083 unique values calculated
     üîç Calculating missing unique count for CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT...
     ‚úÖ CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT: 14 unique values calculated
     üîç Calculating missing unique count for TOTAL_RECURRING_REVENUE_M1_M5_MAX...
     ‚úÖ TOTAL_RECURRING_REVENUE_M1_M5_MAX: 5986 unique values calculated
‚úÖ Combined univariate analysis and Population Stability Index assessment completed
   ‚ôªÔ∏è Reused cardinality and missing values for ALL 20 features
   üìä Overall stability grade: A (95.0/100)
   ‚ö†Ô∏è Stability warnings found: 0
üìà Performing bivariate analysis...
   üìà Analyzing target distribution by FIXED_TENURE_DAYS...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by USAGE_TOTAL_UP_BYTES_M0...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by TOTAL_RECURRING_REVENUE...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by FIXED_DATA_TENURE_DAYS...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by TOTAL_DATA_RECURRING_REVENUE...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by USAGE_TOTAL_DOWN_BYTES_M1...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by HOUSE_COMP_UPLOAD_SPEED_AVG...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by IVR_FIXED_TOTAL_CNT_PREV_30D_CNT...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by VIDEO_RECURRING_REVENUE...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by USAGE_TOTAL_DOWN_BYTES_M0...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by HOUSE_COMP_DOWNLOAD_SPEED_AVG...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by USAGE_TOTAL_UP_BYTES_M1...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by HOUSE_COMP_FW_UPLOAD_SPEED_AVG...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by OVERDUE_BALANCE_OVR...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by FIXED_DATA_UPLOAD_SPEED...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by USAGE_VIDEO_STB_NO_VOD_MINS_M1...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT...
     üìä Processing as numerical feature...
   üìà Analyzing target distribution by TOTAL_RECURRING_REVENUE_M1_M5_MAX...
     üìä Processing as numerical feature...
üîó Computing correlation matrix...
   üîó Computing correlations for 20 features using optimized MLlib approach...
   ‚úÖ Target column 'target' is already numeric
   üìä Assembling feature vectors for 21 columns...
   üßÆ Computing pearson correlation matrix...
   üîç Analyzing feature-feature correlations for multicollinearity...
   ‚úÖ Correlation analysis complete: 27 high correlations found
‚úÖ Feature profiling completed for 20 features
‚úÖ train profiling completed and saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/automl_user_train_feature_profiling.json
‚úÖ Combined profiling results saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/automl_user_all_datasets_feature_profiling.json
üîç Successfully profiled 1 datasets with 30 features
25/09/24 21:47:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:47:05 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
‚úÖ Cross-validation enabled: no out-of-time validation data available

5. Model Building and Validation...
üîÑ Using cross-validation for model training and validation...
Cross-validation dataset size: 78142 samples
üìã Note: CV training uses upsampled data, but evaluation metrics use original data

üìä Cross-Validation Metrics Configuration:
  Binary classification: areaUnderROC
  Multiclass classification: accuracy
  üìà areaUnderROC: Measures ability to distinguish between classes
  üéØ accuracy: Overall classification accuracy
Building xgboost model with 5-fold cross-validation...

üîÑ Training both default and tuned xgboost models with 5-fold CV...
‚úÖ Spark session is healthy
Using areaUnderROC as cross-validation metric for binary classification
  üìä Training default xgboost model with CV...
Creating xgboost estimator for cross-validation...
‚úÖ XGBoost estimator created successfully with parameters: {}
   Features column: features
   Label column: target
xgboost estimator created successfully.
2025-09-24 21:47:11,501 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:47:22,503 INFO XGBoost-PySpark: _fit Finished xgboost training!
25/09/24 21:47:23 INFO BaseAllocator: Debug mode disabled.
25/09/24 21:47:23 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
25/09/24 21:47:23 INFO CheckAllocator: Using DefaultAllocationManager at memory-netty-11.0.0.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class
2025-09-24 21:47:44,455 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:47:48,623 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:03,967 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:08,115 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:23,385 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:27,488 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:42,021 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:46,221 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:49:00,910 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'objective': 'binary:logistic', 'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:49:06,729 INFO XGBoost-PySpark: _fit Finished xgboost training!
Validating xgboost_default model on 3 datasets...
Validating on train dataset...
Model prediction completed
AUC calculated 0.9992806955584262
Accuracy calculated 0.9986675113043099
KS calculation starting
KS_Value = 90.73
Metrics calculation process Completed in :  9.896067142486572 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for train data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model train.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for train data.png
All validation metrics computed
Model validation process completed in: 10.80 seconds
Validating on valid dataset...
Model prediction completed
AUC calculated 0.9606439885978209
Accuracy calculated 0.9912169805043582
KS calculation starting
KS_Value = 76.73
Metrics calculation process Completed in :  5.641307592391968 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for valid data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model valid.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for valid data.png
All validation metrics computed
Model validation process completed in: 6.27 seconds
Validating on test dataset...
Model prediction completed
AUC calculated 0.9777210439615176
Accuracy calculated 0.9922942433465001
KS calculation starting
KS_Value = 83.31
Metrics calculation process Completed in :  5.532714605331421 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for test data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model test.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for test data.png
All validation metrics computed
Model validation process completed in: 6.17 seconds
Metrics saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_metrics.z and /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_metrics.json
Looking for KS files with pattern: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model*.xlsx
Found 3 KS files to process
Sheet name: 'KS xgboost_default Model valid' -> 'XGB_def_valid'
‚úì Conditional formatting applied to XGB_def_valid
‚úì Processed and marked for removal: KS xgboost_default Model valid.xlsx
Sheet name: 'KS xgboost_default Model test' -> 'XGB_def_test'
‚úì Conditional formatting applied to XGB_def_test
‚úì Processed and marked for removal: KS xgboost_default Model test.xlsx
Sheet name: 'KS xgboost_default Model train' -> 'XGB_def_train'
‚úì Conditional formatting applied to XGB_def_train
‚úì Processed and marked for removal: KS xgboost_default Model train.xlsx
‚úì Removed: KS xgboost_default Model valid.xlsx
‚úì Removed: KS xgboost_default Model test.xlsx
‚úì Removed: KS xgboost_default Model train.xlsx
Total files processed: 3
KS charts consolidated in: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS_Charts.xlsx
Note: Conditional formatting should now be visible in the Excel file
Validation summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_validation_summary.txt
xgboost_default model validation completed.
  üìù Hyperparameter optimization disabled - using default xgboost model
Model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_model
‚úÖ xgboost CV model training completed - selected default version
Building lightgbm model with 5-fold cross-validation...

üîÑ Training both default and tuned lightgbm models with 5-fold CV...
‚úÖ Spark session is healthy
Using areaUnderROC as cross-validation metric for binary classification
  üìä Training default lightgbm model with CV...
Creating lightgbm estimator for cross-validation...
lightgbm estimator created successfully.
üîÑ fitMultiple called for Native LightGBM cross-validation
üìä Training model 1/1 for CV fold
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
üìä Training data: 62512 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(60417), np.int64(1): np.int64(2095)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üîÑ fitMultiple called for Native LightGBM cross-validation
üìä Training model 1/1 for CV fold
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üìä Training data: 62754 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(60612), np.int64(1): np.int64(2142)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üîÑ fitMultiple called for Native LightGBM cross-validation
üìä Training model 1/1 for CV fold
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üìä Training data: 62544 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(60394), np.int64(1): np.int64(2150)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üîÑ fitMultiple called for Native LightGBM cross-validation
üìä Training model 1/1 for CV fold
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üìä Training data: 62409 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(60301), np.int64(1): np.int64(2108)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üîÑ fitMultiple called for Native LightGBM cross-validation
üìä Training model 1/1 for CV fold
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üìä Training data: 62337 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(60185), np.int64(1): np.int64(2152)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üöÄ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
üìä Training data: 78142 samples, 28 features
üéØ Number of classes: 2
üìã Class distribution: {np.int64(0): np.int64(75480), np.int64(1): np.int64(2662)}
üîß LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
üèãÔ∏è Training LightGBM model...
‚úÖ Native LightGBM model trained successfully!
üìä Model info: 100 trees, 28 features
Validating lightgbm_default model on 3 datasets...
Validating on train dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.993106127494261
Accuracy calculated 0.994407916293497
KS calculation starting
KS_Value = 88.82
Metrics calculation process Completed in :  10.623364448547363 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for train data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model train.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for train data.png
All validation metrics computed
Model validation process completed in: 29.26 seconds
Validating on valid dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.851038283899834
Accuracy calculated 0.9901523720806441
KS calculation starting
KS_Value = 53.07
Metrics calculation process Completed in :  6.853440284729004 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for valid data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model valid.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for valid data.png
All validation metrics computed
Model validation process completed in: 11.67 seconds
Validating on test dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.8517588520763953
Accuracy calculated 0.9906106326490967
KS calculation starting
KS_Value = 51.43
Metrics calculation process Completed in :  4.860246896743774 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for test data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model test.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for test data.png
All validation metrics computed
Model validation process completed in: 10.01 seconds
Metrics saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_metrics.z and /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_metrics.json
Looking for KS files with pattern: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model*.xlsx
Found 3 KS files to process
Sheet name: 'KS lightgbm_default Model test' -> 'LGB_def_test'
‚úì Conditional formatting applied to LGB_def_test
‚úì Processed and marked for removal: KS lightgbm_default Model test.xlsx
Sheet name: 'KS lightgbm_default Model valid' -> 'LGB_def_valid'
‚úì Conditional formatting applied to LGB_def_valid
‚úì Processed and marked for removal: KS lightgbm_default Model valid.xlsx
Sheet name: 'KS lightgbm_default Model train' -> 'LGB_def_train'
‚úì Conditional formatting applied to LGB_def_train
‚úì Processed and marked for removal: KS lightgbm_default Model train.xlsx
‚úì Removed: KS lightgbm_default Model test.xlsx
‚úì Removed: KS lightgbm_default Model valid.xlsx
‚úì Removed: KS lightgbm_default Model train.xlsx
Total files processed: 3
KS charts consolidated in: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS_Charts.xlsx
Note: Conditional formatting should now be visible in the Excel file
Validation summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_validation_summary.txt
lightgbm_default model validation completed.
  üìù Hyperparameter optimization disabled - using default lightgbm model
Model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
‚úÖ lightgbm CV model training completed - selected default version

üìä Model Building Summary:
   ‚úÖ Successfully built: xgboost, lightgbm
   ‚ùå Failed to build: None

6. Model Selection...
Selecting best model based on ks on valid dataset...
Debug: Created DataFrame with shape: (2, 12)
Debug: DataFrame columns: ['model_type', 'roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'cv_score', 'selection_info']
Debug: DataFrame head:
  model_type  ...                                     selection_info
0    xgboost  ...  {'decision': 'default', 'reasons': ['Hyperpara...
1   lightgbm  ...  {'decision': 'default', 'reasons': ['Hyperpara...

[2 rows x 12 columns]
Debug: best_model_info keys: ['model_type', 'selection_criteria', 'dataset_used', 'performance_score', 'stability_score', 'rank', 'all_results']
Debug: DataFrame columns: ['model_type', 'roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'cv_score', 'selection_info', 'counter', 'selected_model']
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
‚úÖ Model selection results saved to Excel: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_results.xlsx
Selection results saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_results.xlsx
Selection summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_summary.txt
Best model selected: lightgbm
Loaded lightgbm model from /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
‚úÖ Best model loaded successfully: lightgbm
Best model selected: lightgbm

7. Generating Scoring Code...
Generating scoring scripts...
Generated xgboost_model_scoring.py
Generated lightgbm_model_scoring.py
Scoring scripts generated in /tmp/automl_results/job_0002_automl_user_automl_model_1758749709

8. Save model configuration files...
‚úÖ Model info saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_info.pkl
‚úÖ Character labels saved
‚úÖ Pipeline model saved
‚úÖ lightgbm model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
üéâ Complete model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
üìÅ Saved files:
   - model_info.pkl
   - lightgbm_model/
   - char_labels/
   - pipeline_model/

9. Computing SHAP values for best model (lightgbm) explainability...
   üìä Using processed dataset for SHAP: 76,251 rows
   üéØ SHAP will analyze 28 selected features
   üìä SHAP feature breakdown: 28 numerical, 0 categorical
25/09/24 21:52:54 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:55 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  0%|          | 0/50 [00:00<?, ?it/s]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:56 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  2%|‚ñè         | 1/50 [00:03<02:36,  3.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:59 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  4%|‚ñç         | 2/50 [00:05<02:11,  2.74s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:03 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  6%|‚ñå         | 3/50 [00:07<01:57,  2.51s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:04 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:05 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  8%|‚ñä         | 4/50 [00:10<01:54,  2.49s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:06 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:08 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 10%|‚ñà         | 5/50 [00:12<01:49,  2.44s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:08 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:10 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 12%|‚ñà‚ñè        | 6/50 [00:14<01:43,  2.36s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:10 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:12 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 14%|‚ñà‚ñç        | 7/50 [00:17<01:40,  2.35s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:14 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 16%|‚ñà‚ñå        | 8/50 [00:19<01:35,  2.27s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 18%|‚ñà‚ñä        | 9/50 [00:21<01:33,  2.28s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 20%|‚ñà‚ñà        | 10/50 [00:23<01:31,  2.29s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 22%|‚ñà‚ñà‚ñè       | 11/50 [00:25<01:25,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:23 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 24%|‚ñà‚ñà‚ñç       | 12/50 [00:28<01:23,  2.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:24 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:25 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 26%|‚ñà‚ñà‚ñå       | 13/50 [00:30<01:20,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:27 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 28%|‚ñà‚ñà‚ñä       | 14/50 [00:32<01:16,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:29 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 30%|‚ñà‚ñà‚ñà       | 15/50 [00:34<01:15,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:30 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:36<01:13,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:34 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:38<01:11,  2.17s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:34 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:36 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:40<01:09,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:37 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:38 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:43<01:06,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:39 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:40 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:45<01:05,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:41 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:47<01:03,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:43 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:49<01:00,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:47 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:51<00:59,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:47 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:49 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:54<00:57,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:50 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:51 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:56<00:55,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:52 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:53 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:58<00:52,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:54 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:55 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:55 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:55 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [01:00<00:49,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:56 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [01:02<00:47,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:00 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [01:04<00:45,  2.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:02 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [01:07<00:42,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:03 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:04 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [01:09<00:41,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:05 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:06 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [01:11<00:38,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:07 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:09 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [01:13<00:37,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:09 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:11 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [01:15<00:34,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:11 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [01:17<00:31,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [01:19<00:29,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [01:22<00:27,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:18 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [01:24<00:25,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:20 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [01:26<00:23,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:22 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:23 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [01:28<00:21,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:24 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [01:30<00:19,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [01:32<00:17,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:30 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [01:35<00:15,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:31 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [01:37<00:13,  2.24s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:33 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:35 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [01:40<00:11,  2.38s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:36 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:37 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [01:42<00:09,  2.31s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:38 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:39 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [01:44<00:06,  2.29s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:40 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [01:46<00:04,  2.22s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:44 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [01:48<00:02,  2.27s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_

100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:51<00:00,  2.32s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:51<00:00,  2.23s/it]
‚úÖ SHAP summary plot saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/shap_summary_classification.png
‚úÖ SHAP values saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/shap_values_classification.csv
‚úÖ SHAP values computed successfully for best model (lightgbm)

AutoML pipeline completed successfully!
================================================================================
üéØ MODEL SELECTION SUMMARY
================================================================================

üìä Overall Statistics:
   ‚Ä¢ Total models trained: 2
   ‚Ä¢ Default models selected: 2
   ‚Ä¢ Tuned models selected: 0
   ‚Ä¢ Hyperparameter optimization: ‚ùå Disabled

üéõÔ∏è Selection Criteria:
   ‚Ä¢ Minimum improvement threshold: 1.0%
   ‚Ä¢ Maximum overfitting gap: 5.0%
   ‚Ä¢ Statistical significance threshold: 5.0%

üîç Individual Model Decisions:

   üìä XGBOOST: DEFAULT MODEL SELECTED
      üìà Default performance: 76.7300
      üéØ Tuned performance: N/A (no tuning performed)
      üí≠ Key reasons:
         ‚Ä¢ Hyperparameter optimization is disabled

   üìä LIGHTGBM: DEFAULT MODEL SELECTED
      üìà Default performance: 53.0700
      üéØ Tuned performance: N/A (no tuning performed)
      üí≠ Key reasons:
         ‚Ä¢ Hyperparameter optimization is disabled

================================================================================
üí° INTERPRETATION GUIDE:
   üéØ TUNED models: Hyperparameter optimization provided significant benefit
   üìä DEFAULT models: Default parameters performed as well or better
   üé™ Overfitting gap: Difference between training and validation performance
   üìà Higher scores are better for accuracy, AUC, KS; lower for loss metrics
================================================================================

üìÑ Model selection summary saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_summary.txt
[2025-09-24T21:54:47.717188] üìä Progress: 75.0% - AutoML training completed
[2025-09-24T21:54:47.717219] ‚úÖ AutoML.fit() completed successfully
[2025-09-24T21:54:47.717234] üìä Progress: 87.5% - Saving results to GCS...
[2025-09-24T21:54:47.717244] üéâ AutoML pipeline completed successfully!
[2025-09-24T21:54:47.717256] üì§ Copying results from /tmp/automl_results/job_0002_automl_user_automl_model_1758749709 to GCS...
[2025-09-24T21:54:54.190106] ‚úÖ Results successfully copied to gs://rapid_modeler_app/automl_results/job_0002_automl_user_automl_model_1758749709/
[2025-09-24T21:54:54.191015] üìã Creating comprehensive execution logs for Streamlit...
[2025-09-24T21:54:54.191081] ‚ö†Ô∏è Warning: Failed to create job status files: local variable 'sys' referenced before assignment
[2025-09-24T21:54:54.191108] üìä Progress: 100.0% - Job completed successfully
[2025-09-24T21:54:54.191119] üéâ Dataproc Serverless job completed successfully!
[2025-09-24T21:54:54.191457] üö® Emergency shutdown timer set: 10 seconds post-completion
[2025-09-24T21:54:54.191501] üéâ Dataproc Serverless job completed successfully!
[2025-09-24T21:54:54.191554] ‚ö†Ô∏è Warning: Could not mark threads as daemon: cannot set daemon status of active thread
[2025-09-24T21:54:54.191574] üßπ Closing Spark session (graceful)...
[2025-09-24T21:54:55.795348] ‚úÖ Spark session closed successfully
[2025-09-24T21:54:55.795387] üìå Job completed successfully - initiating immediate termination to avoid hang
[2025-09-24T21:54:55.795400] üö™ Implementing enhanced force shutdown mechanism...
[2025-09-24T21:54:55.795652] üö™ Attempting graceful sys.exit...
[2025-09-24T21:54:55.795704] üö™ Attempting os._exit...
