Pulling image us-central1-docker.pkg.dev/atus-prism-dev/ml-repo/rapid_modeler_dataproc:latest
Image is up to date for sha256:c562dfe97cdf1078d319af362186542e70ac5d05c455e9d10289e48fa1f04d81
Waiting for container log creation
PYSPARK_PYTHON=python3
JAVA_HOME=/usr/lib/jvm/temurin-17-jdk-amd64
SPARK_EXTRA_CLASSPATH=
:: loading settings :: file = /etc/spark/conf/ivysettings.xml
[2025-09-24T21:38:42.115288] ğŸ›¡ï¸ Cleanup handlers registered
[2025-09-24T21:38:42.115380] â° Maximum timeout protection set: 180 minutes
[2025-09-24T21:38:42.116061] ğŸš€ Starting Dataproc Serverless job: job_0002_automl_user_automl_model_1758749709
[2025-09-24T21:38:42.116139] ğŸ“Š Batch ID: automl-spark-job-0002-automl-user-automl-model-1758749709-20250
[2025-09-24T21:38:42.116169] ğŸ“ Working directory: /var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673
[2025-09-24T21:38:42.116200] ğŸ“Š Progress: 12.5% - Initializing Spark session...
[2025-09-24T21:39:16.569343] ğŸ§  Spark resource allocation:
[2025-09-24T21:39:18.573508]    Executors: 8
[2025-09-24T21:39:18.574178]    Executor Memory: 32g
[2025-09-24T21:39:18.574891]    Driver Memory: 16g
[2025-09-24T21:39:18.574921] âœ… Spark session initialized for Dataproc Serverless
[2025-09-24T21:39:18.577144] ğŸ“Š Spark UI available at: http://gdpic-srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673-m.us-east1-d.c.atus-prism-dev.internal:4040
[2025-09-24T21:39:18.578824] ğŸ”§ Spark configuration: driver.memory=16g, executor.memory=32g
[2025-09-24T21:39:18.578863] ğŸ“¦ Using custom container image with pre-installed packages...
[2025-09-24T21:39:18.578872] â„¹ï¸ Skipping runtime package installation - all dependencies are pre-installed
[2025-09-24T21:39:18.580243] âœ… Config file found at: /app/automl_pyspark/config.yaml
[2025-09-24T21:39:18.581322] ğŸ“ Results will be saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
[2025-09-24T21:39:18.582144] ğŸ” Debugging mounted directories:
[2025-09-24T21:39:18.583351] ğŸ“‚ /app/automl_pyspark/automl_results/job_0002_automl_user_automl_model_1758749709 does not exist yet
[2025-09-24T21:39:18.584202] ğŸ“‚ Active results_dir /tmp/automl_results/job_0002_automl_user_automl_model_1758749709 contains: []
[2025-09-24T21:39:18.584957] ğŸ“Š Progress: 25.0% - Importing AutoML classes...
[2025-09-24T21:39:18.585244] ğŸ“¦ Importing AutoML classes...
âœ… XGBoost Spark integration available and functional
âœ… Native LightGBM integration available and functional (version: 4.6.0)
âœ… Native LightGBM regression available and functional (version: 4.6.0)
[2025-09-24T21:39:25.441587] âœ… AutoML classes imported successfully
[2025-09-24T21:39:25.441631] ğŸ”§ Applying Dataproc compatibility fixes...
[2025-09-24T21:39:25.441646] âœ… Applied pandas Excel fallback patch
[2025-09-24T21:39:27.427511] âœ… SHAP library is available
[2025-09-24T21:39:27.427551] âœ… Dataproc compatibility fixes applied
[2025-09-24T21:39:27.427571] ğŸ“Š Progress: 37.5% - Initializing AutoML...
[2025-09-24T21:39:27.427583] ğŸ“ Using original data file path: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
[2025-09-24T21:39:27.427590] ğŸ“ Data file: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
[2025-09-24T21:39:27.427596] ğŸ¯ Target column: target
[2025-09-24T21:39:27.427602] ğŸ“‹ Task type: classification
[2025-09-24T21:39:27.427608] ğŸ“‹ Source type: bigquery
[2025-09-24T21:39:27.427613] ğŸ—ï¸ Initializing AutoML Class...
[2025-09-24T21:39:27.427639] ğŸ”§ Using config file: /app/automl_pyspark/config.yaml
âœ… Configuration loaded from: /app/automl_pyspark/config.yaml
ğŸŒ Using environment: development
âœ… Applying development environment configuration
âœ… XGBoost detected and available
âœ… Native LightGBM detected and available (version: 4.6.0)
âœ… Set run_xgboost to True for regression
âœ… Set run_lightgbm to True for regression
ğŸ¯ Applying '' preset configuration (highest priority)...
   ğŸ“ No preset selected - using configuration from YAML file
âœ… XGBoost integration verified - adding to available models
âœ… Native LightGBM integration verified - adding to available models

ğŸ”§ AutoML Configuration Loaded:
   ğŸ“ Config source: /app/automl_pyspark/config.yaml
âœ… Configuration validation passed for classification

ğŸ”§ AutoML Model Configuration:
  Core Models:
    Logistic Regression: âœ… Enabled
    Random Forest: â­• Disabled
    Gradient Boosting: â­• Disabled
    Decision Tree: â­• Disabled
    Neural Network: â­• Disabled
  Advanced Models:
    XGBoost: âœ… Enabled
    LightGBM: â­• Disabled
  Total Models Enabled: 2
  ğŸ’¡ Install XGBoost: pip install xgboost>=1.6.0
  ğŸ’¡ Install LightGBM: pip install synapseml>=0.11.0
[2025-09-24T21:39:27.482855] âœ… AutoML initialized successfully
[2025-09-24T21:39:27.482901] ğŸ“Š Progress: 50.0% - Initializing data manager...
[2025-09-24T21:39:27.495213] âœ… Data manager initialized successfully
[2025-09-24T21:39:27.495281] ğŸ“Š Progress: 62.5% - Running AutoML pipeline...
[2025-09-24T21:39:27.495290] ğŸ¯ Starting classification AutoML pipeline...
[2025-09-24T21:39:27.495300] ğŸ“… Loading OOT1 data from BigQuery: `atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism_oot1`
âš ï¸ BigQuery data loading attempt 1/3 failed: An error occurred while calling o96.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

â³ Retrying in 5 seconds...
âš ï¸ BigQuery data loading attempt 2/3 failed: An error occurred while calling o106.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

â³ Retrying in 10 seconds...
âš ï¸ BigQuery data loading attempt 3/3 failed: An error occurred while calling o116.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

âŒ All 3 attempts failed
[2025-09-24T21:39:53.223024] âš ï¸ OOT1 BigQuery loading failed: Failed to load data from BigQuery after 3 attempts. Last error: An error occurred while calling o116.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot1%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more
 - will skip OOT1
[2025-09-24T21:39:53.223079] ğŸ“… Loading OOT2 data from BigQuery: `atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism_oot2`
âš ï¸ BigQuery data loading attempt 1/3 failed: An error occurred while calling o126.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

â³ Retrying in 5 seconds...
âš ï¸ BigQuery data loading attempt 2/3 failed: An error occurred while calling o136.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

â³ Retrying in 10 seconds...
âš ï¸ BigQuery data loading attempt 3/3 failed: An error occurred while calling o146.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more

âŒ All 3 attempts failed
[2025-09-24T21:40:08.532210] âš ï¸ OOT2 BigQuery loading failed: Failed to load data from BigQuery after 3 attempts. Last error: An error occurred while calling o146.load.
: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:784)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:781)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:780)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)
	at com.google.cloud.bigquery.connector.common.BigQueryClient.getReadTable(BigQueryClient.java:253)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelationInternal(BigQueryRelationProvider.scala:77)
	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:46)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
GET https://www.googleapis.com/bigquery/v2/projects/%60atus-prism-dev/datasets/ds_sandbox/tables/sundar_east_b2c_fixed_churn_train_prism_oot2%60?prettyPrint=false
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
    "reason" : "badRequest"
  } ],
  "message" : "Invalid resource name projects/`atus-prism-dev; Project id: `atus-prism-dev",
  "status" : "INVALID_ARGUMENT"
}
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)
	at com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)
	at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)
	... 28 more
 - will skip OOT2
[2025-09-24T21:40:08.532264] ğŸ”„ OOT1 data is None - will not be passed to AutoML.fit()
[2025-09-24T21:40:08.532277] ğŸ”„ OOT2 data is None - will not be passed to AutoML.fit()
[2025-09-24T21:40:08.533095] ğŸ”„ Calling AutoML.fit() with 28 parameters...
ğŸ“Š Loading training data from: atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
ğŸ” Large dataset detected: 2,596,745 rows
ğŸ“Š Creating sample temp table (100K rows) for feature engineering...
ğŸ”§ Creating sample temp table with query: 
            CREATE TABLE `atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc` AS
    ...
âœ… Sample temp table created: atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc (expires in 24h)
âœ… Using sample temp table for feature engineering: atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc
25/09/24 21:40:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
25/09/24 21:40:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:40:37 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:37 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ“Š Loaded sampled data for feature engineering: 100,000 rows
ğŸ’¡ Full dataset will be loaded after feature selection for training
â±ï¸ Pipeline timeout set to 120 minutes
â±ï¸ Setting pipeline timeout: 120 minutes

ğŸ”§ AutoML Model Configuration:
  Core Models:
    Logistic Regression: â­• Disabled
    Random Forest: â­• Disabled
    Gradient Boosting: â­• Disabled
    Decision Tree: â­• Disabled
    Neural Network: â­• Disabled
  Advanced Models:
    XGBoost: âœ… Enabled
    LightGBM: âœ… Enabled
  Total Models Enabled: 2
  ğŸ’¡ Install XGBoost: pip install xgboost>=1.6.0
  ğŸ’¡ Install LightGBM: pip install synapseml>=0.11.0
Starting AutoML pipeline...
Target column: target
ğŸ“Š Analyzing dataset size...
25/09/24 21:40:38 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:40:38 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
Training data shape: 100,000 rows, 300 columns
ğŸ“Š Dataset size stored: large
ğŸ” Detecting number of target classes...
25/09/24 21:40:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:40:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHNTaTVJV28yVU5RcRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:40:42 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHNTaTVJV28yVU5RcRoCamkaAmpm
âœ… Detected 2 classes. Multiclass: False

1. Data Preprocessing...
Starting data preprocessing...
ğŸ“Š Using full dataset (sample_fraction = 1.0)
ğŸ” Analyzing target column classes...
25/09/24 21:41:12 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:41:12 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGpHeXJ2dllNdldjVBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:12 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGpHeXJ2dllNdldjVBoCamkaAmpm
ğŸ“Š Detected 2 classes using distinct count
Initial feature count: 297
ğŸ—“ï¸ Automatically filtered out 1 date/timestamp columns:
   - SNAPSHOT_DAY_DESC (Spark date)
ğŸ’¡ Date columns are excluded because they often don't provide meaningful features for ML models
Features after date column filtering: 296
25/09/24 21:41:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:41:26 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:41:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,HOUSE_MDU_IND,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,CUSTOMER_FIBER_TENURE_MONTH,HAS_DATA_IND,HAS_VIDEO_IND,HAS_VOICE_IND,HAS_MOBILE_IND,FTTH_IND,BAC_IND,ACP_IND,OPTIMUM_COMPLETE_IND,PREMIUM_HBO_IND,PREMIUM_MAX_IND,STREAM_IND,FIXED_SUB_AUTO_PAY_IND,FIXED_CUSTOMER_EBILL_STATUS,PIA_PAID_IND,CARE_FIBER_OFFER_ELIGIBLE_IND,RETENTION_FIBER_OFFER_ELIGIBLE_IND,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,SPEED_ADDED_PREV_30D_IND,SPEED_ADDED_PREV_60D_IND,SPEED_ADDED_PREV_90D_IND,VIDEO_TIERS_ADDED_PREV_30D_IND,VIDEO_TIERS_ADDED_PREV_60D_IND,VIDEO_TIERS_ADDED_PREV_90D_IND,UPGRADE_IND_PREV_30D_IND,UPGRADE_IND_PREV_60D_IND,UPGRADE_IND_PREV_90D_IND,DATA_PSU_UPGRADES_PREV_30D_IND,DATA_PSU_UPGRADES_PREV_60D_IND,DATA_PSU_UPGRADES_PREV_90D_IND,VIDEO_PSU_UPGRADES_PREV_30D_IND,VIDEO_PSU_UPGRADES_PREV_60D_IND,VIDEO_PSU_UPGRADES_PREV_90D_IND,VOICE_PSU_UPGRADES_PREV_30D_IND,VOICE_PSU_UPGRADES_PREV_60D_IND,VOICE_PSU_UPGRADES_PREV_90D_IND,DOWNGRADE_IND_PREV_30D_IND,DOWNGRADE_IND_PREV_60D_IND,DOWNGRADE_IND_PREV_90D_IND,DATA_PSU_DOWNGRADES_PREV_30D_IND,DATA_PSU_DOWNGRADES_PREV_60D_IND,DATA_PSU_DOWNGRADES_PREV_90D_IND,VIDEO_PSU_DOWNGRADES_PREV_30D_IND,VIDEO_PSU_DOWNGRADES_PREV_60D_IND,VIDEO_PSU_DOWNGRADES_PREV_90D_IND,VOICE_PSU_DOWNGRADES_PREV_30D_IND,VOICE_PSU_DOWNGRADES_PREV_60D_IND,VOICE_PSU_DOWNGRADES_PREV_90D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_VOD_MINS_M0,USAGE_VIDEO_STB_DVR_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_VOICE_DOM_SECONDS_M0,USAGE_VOICE_DOM_CALLS_COUNT_M0,USAGE_SMS_COUNT_M0,USAGE_MMS_CNT_M0,USAGE_WIFI_INPUT_BYTES_M0,USAGE_WIFI_OUTPUT_BYTES_M0,USAGE_WIFI_DEVICES_COUNT_M0,USAGE_OPTIMUM_APP_VISIT_COUNT_M0,USAGE_OPTIMUM_APP_PAGE_COUNT_M0,USAGE_OPTIMUM_WEBSITE_VISIT_COUNT_M0,USAGE_OPTIMUM_WEBSITE_PAGE_COUNT_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,USAGE_VIDEO_STB_VOD_MINS_M1,USAGE_VIDEO_STB_DVR_MINS_M1,USAGE_WIFI_INPUT_BYTES_M1,USAGE_WIFI_OUTPUT_BYTES_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,FIXED_DATA_ROLLOFF_MONTH_LEFT,FIXED_VIDEO_ROLLOFF_MONTH_LEFT,FIXED_VOICE_ROLLOFF_MONTH_LEFT,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,BILLING_COLLECTION_STATUS_DESC,OVERDUE_BALANCE_30D,OVERDUE_BALANCE_60D,OVERDUE_BALANCE_90D,OVERDUE_BALANCE_120D,OVERDUE_BALANCE_OVR,AMT_PAY_IN_COLLECTIONS_PAST30D,AMT_PAY_IN_COLLECTIONS_PAST60D,AMT_PAY_IN_COLLECTIONS_PAST90D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST30D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST60D,CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST90D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST30D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST60D,CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST90D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D,CNT_PAY_IN_COLLECTIONS_PAST30D,CNT_PAY_IN_COLLECTIONS_PAST60D,CNT_PAY_IN_COLLECTIONS_PAST90D,DAYS_COLLECTIONS_PENDING_NPD_PAST30D,DAYS_COLLECTIONS_PENDING_NPD_PAST60D,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,DAYS_IN_COLLECTIONS_PAST30D,DAYS_IN_COLLECTIONS_PAST60D,DAYS_IN_COLLECTIONS_PAST90D,WO_RATE_EVENT_IND_PREV_60D_CNT,WO_RATE_EVENT_IND_PREV_180D_CNT,WO_PROMO_ROLLOFF_IND_PREV_60D_CNT,WO_PROMO_ROLLOFF_IND_PREV_180D_CNT,RATE_TOTAL_LIFT_AMT_M0,RATE_DATA_LIFT_AMT_M0,RATE_VIDEO_LIFT_AMT_M0,RATE_VOICE_LIFT_AMT_M0,RATE_TOTAL_LIFT_AMT_M1,RATE_DATA_LIFT_AMT_M1,RATE_VIDEO_LIFT_AMT_M1,RATE_VOICE_LIFT_AMT_M1,RATE_TOTAL_LIFT_AMT_M2,RATE_DATA_LIFT_AMT_M2,RATE_VIDEO_LIFT_AMT_M2,RATE_VOICE_LIFT_AMT_M2,CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT,IVR_FIXED_TOTAL_CNT_PREV_7D_CNT,IVR_CSR_FIXED_CNT_PREV_7D_CNT,IVR_TSR_FIXED_CNT_PREV_7D_CNT,IVR_SALES_FIXED_CNT_PREV_7D_CNT,IVR_RETENTION_FIXED_CNT_PREV_7D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_CSR_FIXED_CNT_PREV_30D_CNT,IVR_TSR_FIXED_CNT_PREV_30D_CNT,IVR_SALES_FIXED_CNT_PREV_30D_CNT,IVR_RETENTION_FIXED_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_60D_CNT,IVR_TSR_FIXED_CNT_PREV_60D_CNT,IVR_SALES_FIXED_CNT_PREV_60D_CNT,IVR_RETENTION_FIXED_CNT_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_90D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,IVR_SALES_FIXED_CNT_PREV_90D_CNT,IVR_RETENTION_FIXED_CNT_PREV_90D_CNT,TRUCK_ROLL_TOTAL_PREV_7D_CNT,TRUCK_ROLL_TOTAL_PREV_30D_CNT,TRUCK_ROLL_TOTAL_PREV_60D_CNT,TRUCK_ROLL_TOTAL_PREV_90D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_7D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_30D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_60D_CNT,TRUCK_ROLL_TOTAL_COMPLETED_PREV_90D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_7D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_30D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_60D_CNT,TRUCK_ROLL_TOTAL_NOT_DONE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_INSTALL_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_CHANGE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_90D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_7D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_30D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_60D_CNT,TRUCK_ROLL_COMPLETED_REPEAT_PREV_90D_CNT,HOUSE_COAX_COMP,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_UPLOAD_SPEED_LOWEST,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_VENDOR_CNT,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,FIXED_CONTACT_CENTER_NPS_RESPONSE_VALUE,FIXED_FIELD_OPS_NPS_RESPONSE_VALUE,FIXED_IVR_NPS_RESPONSE_VALUE,FIXED_OPTIMUM_ADVANCED_SUPPORT_NPS_RESPONSE_VALUE,FIXED_PTS_NPS_RESPONSE_VALUE,FIXED_RETENTION_NPS_RESPONSE_VALUE,FIXED_SALES_NPS_RESPONSE_VALUE,FIXED_SELF_INSTALL_NPS_RESPONSE_VALUE,FIXED_STORE_NPS_RESPONSE_VALUE],|filters=[]
25/09/24 21:41:27 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDMxZVhhT3lHd1dGMxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:27 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDMxZVhhT3lHd1dGMxoCamkaAmpm
Features after missing value filtering: 213
['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']
Categorical variables: 34
Numerical variables: 179
ğŸ” Analyzing cardinality for 34 categorical variables...
ğŸ“Š Computing cardinality and missing values for all 34 variables in batch operations...
25/09/24 21:41:38 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:41:39 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHc2bXVRM3ZwT2o0ZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:41:39 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHc2bXVRM3ZwT2o0ZBoCamkaAmpm
25/09/24 21:41:42 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB
25/09/24 21:42:05 WARN DAGScheduler: Broadcasting large task binary with size 24.4 MiB
25/09/24 21:42:16 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION,OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,HEAD_END_DESC,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:42:17 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHllTFBmVmdZOF8wMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:17 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHllTFBmVmdZOF8wMRoCamkaAmpm
25/09/24 21:42:27 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:42:27 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
   ğŸ“Š OMS_DIVISION: ~1 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š OMS_AREA: ~3 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š OMS_REGION: ~5 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š OMS_MARKET: ~46 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š HOUSE_CLEANSED_STATE: ~5 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š HEAD_END_DESC: ~81 unique values â†’ DROPPING (exceeds threshold of 50)
   ğŸ“Š DWELL_CODE: ~36 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_AGE_RANGE: ~5 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_ECOHORT_GROUP_DESC: ~17 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_SPENDING_TYPE_DESC: ~6 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_CREDIT_LINE_TYPE_DESC: ~6 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_CREDIT_USAGE_TYPE_DESC: ~6 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_LIFESTAGE_DESC: ~49 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_ARCHETYPE_DESC: ~9 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_ACCOUNT_STATUS: ~2 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_ACTIVE_RESI_BUS_DESC: ~1 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_CUSTOMER_EBILL_STATUS: ~3 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BOX_EQUIP_CLASS: ~4 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BOX_EQUIP_CLASS_GROUP: ~3 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_DATA_PRICEPOINT_TIER_DESC: ~27 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_VIDEO_TIER_DESC: ~18 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_VOICE_PRICEPOINT_TIER_DESC: ~14 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0: ~27 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_VIDEO_OFFER_DESC_M0: ~47 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0: ~9 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1: ~29 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_VIDEO_OFFER_DESC_M1: ~49 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1: ~9 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2: ~29 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_VIDEO_OFFER_DESC_M2: ~49 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2: ~9 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_COLLECTION_STATUS_DESC: ~2 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š HOUSE_COPPER_COMP: ~3 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š HOUSE_FIBER_COMP: ~4 unique values â†’ Keeping as CATEGORICAL
ğŸ—‘ï¸ Dropping 1 high-cardinality categorical variables
âœ… Keeping 33 low-cardinality categorical variables
ğŸ—‘ï¸ Removing 1 high-cardinality categorical variables...
ğŸ’¡ These variables have >50 unique values and would create too many features after encoding
   Dropping columns: HEAD_END_DESC
âœ… High-cardinality categorical variables removed from dataset
ğŸ“Š This prevents curse of dimensionality and improves model performance
Updated categorical variables: 33
Numerical variables unchanged: 179
ğŸ”„ Converting string columns to numeric where needed...
â™»ï¸ Reusing cardinality stats for 33 categorical features...
âœ… Using pre-calculated cardinality statistics (threshold: 200 unique values)
   â™»ï¸ OMS_DIVISION: 1 unique values (reused)
   â™»ï¸ OMS_AREA: 3 unique values (reused)
   â™»ï¸ OMS_REGION: 5 unique values (reused)
   â™»ï¸ OMS_MARKET: 46 unique values (reused)
   â™»ï¸ HOUSE_CLEANSED_STATE: 5 unique values (reused)
   â™»ï¸ DWELL_CODE: 36 unique values (reused)
   â™»ï¸ DEMOS_AGE_RANGE: 5 unique values (reused)
   â™»ï¸ DEMOS_ECOHORT_GROUP_DESC: 17 unique values (reused)
   â™»ï¸ DEMOS_SPENDING_TYPE_DESC: 6 unique values (reused)
   â™»ï¸ DEMOS_CREDIT_LINE_TYPE_DESC: 6 unique values (reused)
   â™»ï¸ DEMOS_CREDIT_USAGE_TYPE_DESC: 6 unique values (reused)
   â™»ï¸ DEMOS_LIFESTAGE_DESC: 49 unique values (reused)
   â™»ï¸ DEMOS_ARCHETYPE_DESC: 9 unique values (reused)
   â™»ï¸ FIXED_ACCOUNT_STATUS: 2 unique values (reused)
   â™»ï¸ FIXED_ACTIVE_RESI_BUS_DESC: 1 unique values (reused)
   â™»ï¸ FIXED_CUSTOMER_EBILL_STATUS: 3 unique values (reused)
   â™»ï¸ BOX_EQUIP_CLASS: 4 unique values (reused)
   â™»ï¸ BOX_EQUIP_CLASS_GROUP: 3 unique values (reused)
   â™»ï¸ FIXED_DATA_PRICEPOINT_TIER_DESC: 27 unique values (reused)
   â™»ï¸ FIXED_VIDEO_TIER_DESC: 18 unique values (reused)
   â™»ï¸ FIXED_VOICE_PRICEPOINT_TIER_DESC: 14 unique values (reused)
   â™»ï¸ BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0: 27 unique values (reused)
   â™»ï¸ BILLING_VIDEO_OFFER_DESC_M0: 47 unique values (reused)
   â™»ï¸ BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0: 9 unique values (reused)
   â™»ï¸ BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1: 29 unique values (reused)
   â™»ï¸ BILLING_VIDEO_OFFER_DESC_M1: 49 unique values (reused)
   â™»ï¸ BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1: 9 unique values (reused)
   â™»ï¸ BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2: 29 unique values (reused)
   â™»ï¸ BILLING_VIDEO_OFFER_DESC_M2: 49 unique values (reused)
   â™»ï¸ BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2: 9 unique values (reused)
   â™»ï¸ BILLING_COLLECTION_STATUS_DESC: 2 unique values (reused)
   â™»ï¸ HOUSE_COPPER_COMP: 3 unique values (reused)
   â™»ï¸ HOUSE_FIBER_COMP: 4 unique values (reused)
ğŸ“ˆ Categorical feature filtering: 33 â†’ 33 features (optimized)
25/09/24 21:42:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_DIVISION],|filters=[]
25/09/24 21:42:29 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDER2NkxTMzVnT2dYMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:29 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDER2NkxTMzVnT2dYMhoCamkaAmpm
25/09/24 21:42:33 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_AREA],|filters=[]
25/09/24 21:42:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDENkMWtpSXdEZHNPVRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDENkMWtpSXdEZHNPVRoCamkaAmpm
25/09/24 21:42:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_REGION],|filters=[]
25/09/24 21:42:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFk5Rl9ZeXJuQlQyLRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFk5Rl9ZeXJuQlQyLRoCamkaAmpm
25/09/24 21:42:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_MARKET],|filters=[]
25/09/24 21:42:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDE0ySG9QQnBXYmF6OBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:43 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDE0ySG9QQnBXYmF6OBoCamkaAmpm
25/09/24 21:42:51 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_CLEANSED_STATE],|filters=[]
25/09/24 21:42:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGR0MzBNay12bG1oSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGR0MzBNay12bG1oSxoCamkaAmpm
25/09/24 21:42:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE],|filters=[]
25/09/24 21:42:56 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDC0zcHRZRkx0ZXFPNBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:42:56 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDC0zcHRZRkx0ZXFPNBoCamkaAmpm
25/09/24 21:42:59 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_AGE_RANGE],|filters=[]
25/09/24 21:43:00 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJOWUZsS1lFOGpFRBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:00 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJOWUZsS1lFOGpFRBoCamkaAmpm
25/09/24 21:43:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_ECOHORT_GROUP_DESC],|filters=[]
25/09/24 21:43:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFlxdWd0RDRRaEdKdhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFlxdWd0RDRRaEdKdhoCamkaAmpm
25/09/24 21:43:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_SPENDING_TYPE_DESC],|filters=[]
25/09/24 21:43:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGFhcS1ZWUdxMUZQNhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGFhcS1ZWUdxMUZQNhoCamkaAmpm
25/09/24 21:43:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_CREDIT_LINE_TYPE_DESC],|filters=[]
25/09/24 21:43:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDYwd2ZTSkxNQXNmMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDYwd2ZTSkxNQXNmMRoCamkaAmpm
25/09/24 21:43:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_CREDIT_USAGE_TYPE_DESC],|filters=[]
25/09/24 21:43:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGFrRU90X3JrZ0tvbBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGFrRU90X3JrZ0tvbBoCamkaAmpm
25/09/24 21:43:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC],|filters=[]
25/09/24 21:43:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJfUVp3TjMyck40MxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:08 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJfUVp3TjMyck40MxoCamkaAmpm
25/09/24 21:43:08 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DEMOS_ARCHETYPE_DESC],|filters=[]
25/09/24 21:43:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGJrbDdTSGpsRjI3cRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:08 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGJrbDdTSGpsRjI3cRoCamkaAmpm
25/09/24 21:43:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_ACCOUNT_STATUS],|filters=[]
25/09/24 21:43:09 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGRIRmxYcVFDenhsRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:09 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGRIRmxYcVFDenhsRxoCamkaAmpm
25/09/24 21:43:13 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_ACTIVE_RESI_BUS_DESC],|filters=[]
25/09/24 21:43:13 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHZaS2NzMFJZV1NZVRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:13 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHZaS2NzMFJZV1NZVRoCamkaAmpm
25/09/24 21:43:13 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_CUSTOMER_EBILL_STATUS],|filters=[]
25/09/24 21:43:14 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFNsRnlTdi1IeENkTBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:14 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFNsRnlTdi1IeENkTBoCamkaAmpm
25/09/24 21:43:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:43:15 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFlGdG1nMU1BMHoyZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:15 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFlGdG1nMU1BMHoyZBoCamkaAmpm
25/09/24 21:43:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS_GROUP],|filters=[]
25/09/24 21:43:16 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGswdW0zZnlUU0EwXxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:16 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGswdW0zZnlUU0EwXxoCamkaAmpm
25/09/24 21:43:24 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_DATA_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:43:24 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGllUl82bXpHbDdOaBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:24 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGllUl82bXpHbDdOaBoCamkaAmpm
25/09/24 21:43:25 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_VIDEO_TIER_DESC],|filters=[]
25/09/24 21:43:25 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEJQVlhmSlMtQm9KeBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:25 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEJQVlhmSlMtQm9KeBoCamkaAmpm
25/09/24 21:43:26 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[FIXED_VOICE_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:43:26 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGQ2SmlFNkVONVYwMRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:26 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGQ2SmlFNkVONVYwMRoCamkaAmpm
25/09/24 21:43:27 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:43:27 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGx0U3hRV2hRZ0JNURoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:27 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGx0U3hRV2hRZ0JNURoCamkaAmpm
25/09/24 21:43:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M0],|filters=[]
25/09/24 21:43:28 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGVLbHVjVVdLNC02UhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:28 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGVLbHVjVVdLNC02UhoCamkaAmpm
25/09/24 21:43:28 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:43:29 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZUS2l0S0NnT3AxMBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:29 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDZUS2l0S0NnT3AxMBoCamkaAmpm
25/09/24 21:43:29 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1],|filters=[]
25/09/24 21:43:30 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFdTMGtKcjV0Y2pXcRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:30 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFdTMGtKcjV0Y2pXcRoCamkaAmpm
25/09/24 21:43:30 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M1],|filters=[]
25/09/24 21:43:30 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG10QjhRYTJwZFJDThoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:30 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDG10QjhRYTJwZFJDThoCamkaAmpm
25/09/24 21:43:31 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1],|filters=[]
25/09/24 21:43:31 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGhGTkxIbFBfMmcybBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:31 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGhGTkxIbFBfMmcybBoCamkaAmpm
25/09/24 21:43:32 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2],|filters=[]
25/09/24 21:43:32 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHBpQ1hraFVsVTVPRBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:32 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHBpQ1hraFVsVTVPRBoCamkaAmpm
25/09/24 21:43:33 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M2],|filters=[]
25/09/24 21:43:33 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGpkQnpDdVN6SEpnSRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:33 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGpkQnpDdVN6SEpnSRoCamkaAmpm
25/09/24 21:43:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2],|filters=[]
25/09/24 21:43:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGxBWUIxR3FHeURLNRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:34 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGxBWUIxR3FHeURLNRoCamkaAmpm
25/09/24 21:43:34 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_COLLECTION_STATUS_DESC],|filters=[]
25/09/24 21:43:35 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDBzSFFEdnJqRVJwLRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:35 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDBzSFFEdnJqRVJwLRoCamkaAmpm
25/09/24 21:43:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_COPPER_COMP],|filters=[]
25/09/24 21:43:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG5ud3pkMlVaekdDYRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDG5ud3pkMlVaekdDYRoCamkaAmpm
25/09/24 21:43:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_FIBER_COMP],|filters=[]
25/09/24 21:43:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEI1UmRwWjZteVhTSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEI1UmRwWjZteVhTSxoCamkaAmpm
Categorical encoding step complete.
Numerical impuation step complete.
Categorical encoding complete with _encoded suffix.
Target and Preprocessed features joined.
Final Data Summary - 
âš ï¸ Spark session recovery utilities not available, using basic error handling
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:39 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:39 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ“‹ Cached schema with 213 columns for recovery
ED_HIGHEST': 'bigint', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST': 'bigint', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG': 'double', 'OMS_DIVISION_encoded': 'double', 'OMS_AREA_encoded': 'double', 'OMS_REGION_encoded': 'double', 'OMS_MARKET_encoded': 'double', 'HOUSE_CLEANSED_STATE_encoded': 'double', 'DWELL_CODE_encoded': 'double', 'DEMOS_AGE_RANGE_encoded': 'double', 'DEMOS_ECOHORT_GROUP_DESC_encoded': 'double', 'DEMOS_SPENDING_TYPE_DESC_encoded': 'double', 'DEMOS_CREDIT_LINE_TYPE_DESC_encoded': 'double', 'DEMOS_CREDIT_USAGE_TYPE_DESC_encoded': 'double', 'DEMOS_LIFESTAGE_DESC_encoded': 'double', 'DEMOS_ARCHETYPE_DESC_encoded': 'double', 'FIXED_ACCOUNT_STATUS_encoded': 'double', 'FIXED_ACTIVE_RESI_BUS_DESC_encoded': 'double', 'FIXED_CUSTOMER_EBILL_STATUS_encoded': 'double', 'BOX_EQUIP_CLASS_encoded': 'double', 'BOX_EQUIP_CLASS_GROUP_encoded': 'double', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded': 'double', 'FIXED_VIDEO_TIER_DESC_encoded': 'double', 'FIXED_VOICE_PRICEPOINT_TIER_DESC_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M0_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M1_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1_encoded': 'double', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2_encoded': 'double', 'BILLING_VIDEO_OFFER_DESC_M2_encoded': 'double', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2_encoded': 'double', 'BILLING_COLLECTION_STATUS_DESC_encoded': 'double', 'HOUSE_COPPER_COMP_encoded': 'double', 'HOUSE_FIBER_COMP_encoded': 'double', 'target': 'bigint'}}
Data preprocessing completed.
Feature variables: ['SNAPSHOT_DAY_DESC', 'OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'CUSTOMER_FIBER_TENURE_MONTH', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_VOICE_DOM_SECONDS_M0', 'USAGE_VOICE_DOM_CALLS_COUNT_M0', 'USAGE_SMS_COUNT_M0', 'USAGE_MMS_CNT_M0', 'USAGE_WIFI_INPUT_BYTES_M0', 'USAGE_WIFI_OUTPUT_BYTES_M0', 'USAGE_WIFI_DEVICES_COUNT_M0', 'USAGE_OPTIMUM_APP_VISIT_COUNT_M0', 'USAGE_OPTIMUM_APP_PAGE_COUNT_M0', 'USAGE_OPTIMUM_WEBSITE_VISIT_COUNT_M0', 'USAGE_OPTIMUM_WEBSITE_PAGE_COUNT_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'USAGE_WIFI_INPUT_BYTES_M1', 'USAGE_WIFI_OUTPUT_BYTES_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'FIXED_DATA_ROLLOFF_MONTH_LEFT', 'FIXED_VIDEO_ROLLOFF_MONTH_LEFT', 'FIXED_VOICE_ROLLOFF_MONTH_LEFT', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'AMT_PAY_IN_COLLECTIONS_PAST30D', 'AMT_PAY_IN_COLLECTIONS_PAST60D', 'AMT_PAY_IN_COLLECTIONS_PAST90D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST30D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST60D', 'CNT_COLLECTIONS_INBOUND_CALLS_HANDLED_PAST90D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST30D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST60D', 'CNT_COLLECTIONS_INBOUND_CALLS_OFFERED_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'CNT_PAY_IN_COLLECTIONS_PAST30D', 'CNT_PAY_IN_COLLECTIONS_PAST60D', 'CNT_PAY_IN_COLLECTIONS_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'RATE_TOTAL_LIFT_AMT_M0', 'RATE_DATA_LIFT_AMT_M0', 'RATE_VIDEO_LIFT_AMT_M0', 'RATE_VOICE_LIFT_AMT_M0', 'RATE_TOTAL_LIFT_AMT_M1', 'RATE_DATA_LIFT_AMT_M1', 'RATE_VIDEO_LIFT_AMT_M1', 'RATE_VOICE_LIFT_AMT_M1', 'RATE_TOTAL_LIFT_AMT_M2', 'RATE_DATA_LIFT_AMT_M2', 'RATE_VIDEO_LIFT_AMT_M2', 'RATE_VOICE_LIFT_AMT_M2', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_COMPLETED_PREV_90D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_7D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_30D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_60D_CNT', 'TRUCK_ROLL_TOTAL_NOT_DONE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_INSTALL_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_SERVICE_VISIT_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_CHANGE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_SELF_INSTALL_RESCUE_PREV_90D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_7D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_30D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_60D_CNT', 'TRUCK_ROLL_COMPLETED_REPEAT_PREV_90D_CNT', 'HOUSE_COAX_COMP', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'FIXED_CONTACT_CENTER_NPS_RESPONSE_VALUE', 'FIXED_FIELD_OPS_NPS_RESPONSE_VALUE', 'FIXED_IVR_NPS_RESPONSE_VALUE', 'FIXED_OPTIMUM_ADVANCED_SUPPORT_NPS_RESPONSE_VALUE', 'FIXED_PTS_NPS_RESPONSE_VALUE', 'FIXED_RETENTION_NPS_RESPONSE_VALUE', 'FIXED_SALES_NPS_RESPONSE_VALUE', 'FIXED_SELF_INSTALL_NPS_RESPONSE_VALUE', 'FIXED_STORE_NPS_RESPONSE_VALUE']
Selected variables: ['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'HEAD_END_DESC', 'DWELL_CODE', 'HOUSE_MDU_IND', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'FIXED_CUSTOMER_EBILL_STATUS', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'BILLING_COLLECTION_STATUS_DESC', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']
Categorical variables: ['OMS_DIVISION', 'OMS_AREA', 'OMS_REGION', 'OMS_MARKET', 'HOUSE_CLEANSED_STATE', 'DWELL_CODE', 'DEMOS_AGE_RANGE', 'DEMOS_ECOHORT_GROUP_DESC', 'DEMOS_SPENDING_TYPE_DESC', 'DEMOS_CREDIT_LINE_TYPE_DESC', 'DEMOS_CREDIT_USAGE_TYPE_DESC', 'DEMOS_LIFESTAGE_DESC', 'DEMOS_ARCHETYPE_DESC', 'FIXED_ACCOUNT_STATUS', 'FIXED_ACTIVE_RESI_BUS_DESC', 'FIXED_CUSTOMER_EBILL_STATUS', 'BOX_EQUIP_CLASS', 'BOX_EQUIP_CLASS_GROUP', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'FIXED_VIDEO_TIER_DESC', 'FIXED_VOICE_PRICEPOINT_TIER_DESC', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0', 'BILLING_VIDEO_OFFER_DESC_M0', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1', 'BILLING_VIDEO_OFFER_DESC_M1', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1', 'BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2', 'BILLING_VIDEO_OFFER_DESC_M2', 'BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2', 'BILLING_COLLECTION_STATUS_DESC', 'HOUSE_COPPER_COMP', 'HOUSE_FIBER_COMP']
Numerical variables: ['HOUSE_MDU_IND', 'FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'HAS_DATA_IND', 'HAS_VIDEO_IND', 'HAS_VOICE_IND', 'HAS_MOBILE_IND', 'FTTH_IND', 'BAC_IND', 'ACP_IND', 'OPTIMUM_COMPLETE_IND', 'PREMIUM_HBO_IND', 'PREMIUM_MAX_IND', 'STREAM_IND', 'FIXED_SUB_AUTO_PAY_IND', 'PIA_PAID_IND', 'CARE_FIBER_OFFER_ELIGIBLE_IND', 'RETENTION_FIBER_OFFER_ELIGIBLE_IND', 'FIXED_DATA_UPLOAD_SPEED', 'FIXED_DATA_DOWNLOAD_SPEED', 'SPEED_ADDED_PREV_30D_IND', 'SPEED_ADDED_PREV_60D_IND', 'SPEED_ADDED_PREV_90D_IND', 'VIDEO_TIERS_ADDED_PREV_30D_IND', 'VIDEO_TIERS_ADDED_PREV_60D_IND', 'VIDEO_TIERS_ADDED_PREV_90D_IND', 'UPGRADE_IND_PREV_30D_IND', 'UPGRADE_IND_PREV_60D_IND', 'UPGRADE_IND_PREV_90D_IND', 'DATA_PSU_UPGRADES_PREV_30D_IND', 'DATA_PSU_UPGRADES_PREV_60D_IND', 'DATA_PSU_UPGRADES_PREV_90D_IND', 'VIDEO_PSU_UPGRADES_PREV_30D_IND', 'VIDEO_PSU_UPGRADES_PREV_60D_IND', 'VIDEO_PSU_UPGRADES_PREV_90D_IND', 'VOICE_PSU_UPGRADES_PREV_30D_IND', 'VOICE_PSU_UPGRADES_PREV_60D_IND', 'VOICE_PSU_UPGRADES_PREV_90D_IND', 'DOWNGRADE_IND_PREV_30D_IND', 'DOWNGRADE_IND_PREV_60D_IND', 'DOWNGRADE_IND_PREV_90D_IND', 'DATA_PSU_DOWNGRADES_PREV_30D_IND', 'DATA_PSU_DOWNGRADES_PREV_60D_IND', 'DATA_PSU_DOWNGRADES_PREV_90D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_30D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_60D_IND', 'VIDEO_PSU_DOWNGRADES_PREV_90D_IND', 'VOICE_PSU_DOWNGRADES_PREV_30D_IND', 'VOICE_PSU_DOWNGRADES_PREV_60D_IND', 'VOICE_PSU_DOWNGRADES_PREV_90D_IND', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M0', 'USAGE_VIDEO_STB_VOD_MINS_M0', 'USAGE_VIDEO_STB_DVR_MINS_M0', 'USAGE_VIDEO_STB_INTL_MINS_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'USAGE_VIDEO_STB_VOD_MINS_M1', 'USAGE_VIDEO_STB_DVR_MINS_M1', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'VOICE_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX', 'VIDEO_RECURRING_REVENUE_M1_M5_MAX', 'VOICE_RECURRING_REVENUE_M1_M5_MAX', 'OVERDUE_BALANCE_30D', 'OVERDUE_BALANCE_60D', 'OVERDUE_BALANCE_90D', 'OVERDUE_BALANCE_120D', 'OVERDUE_BALANCE_OVR', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D', 'CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D', 'CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST30D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST60D', 'DAYS_COLLECTIONS_PENDING_NPD_PAST90D', 'DAYS_IN_COLLECTIONS_PAST30D', 'DAYS_IN_COLLECTIONS_PAST60D', 'DAYS_IN_COLLECTIONS_PAST90D', 'WO_RATE_EVENT_IND_PREV_60D_CNT', 'WO_RATE_EVENT_IND_PREV_180D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_60D_CNT', 'WO_PROMO_ROLLOFF_IND_PREV_180D_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT', 'CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT', 'CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT', 'CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_7D_CNT', 'IVR_CSR_FIXED_CNT_PREV_7D_CNT', 'IVR_TSR_FIXED_CNT_PREV_7D_CNT', 'IVR_SALES_FIXED_CNT_PREV_7D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_7D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'IVR_CSR_FIXED_CNT_PREV_30D_CNT', 'IVR_TSR_FIXED_CNT_PREV_30D_CNT', 'IVR_SALES_FIXED_CNT_PREV_30D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_30D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_60D_CNT', 'IVR_CSR_FIXED_CNT_PREV_60D_CNT', 'IVR_TSR_FIXED_CNT_PREV_60D_CNT', 'IVR_SALES_FIXED_CNT_PREV_60D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_90D_CNT', 'IVR_CSR_FIXED_CNT_PREV_90D_CNT', 'IVR_TSR_FIXED_CNT_PREV_90D_CNT', 'IVR_SALES_FIXED_CNT_PREV_90D_CNT', 'IVR_RETENTION_FIXED_CNT_PREV_90D_CNT', 'HOUSE_COMP_VENDOR_CNT', 'HOUSE_COMP_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_VENDOR_CNT', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG', 'HOUSE_COMP_FW_VENDOR_CNT', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']

2. Feature Selection...
ğŸ“Š Available features: 212, Configured max: 50
ğŸ¯ Will select: 50 features
Feature selection: selecting top 50 features...
ğŸ“Š Total features available: 212
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:40 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:40 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ“Š Dataset size: 100,000 rows
ğŸ“Š Dataset category: large
ğŸ“Š Using full dataset (100,000 rows) for feature selection
ğŸ”„ Large feature set detected (212 > 200). Using sequential feature selection...
Processing 212 features in chunks of 100
Created 3 chunks

Processing chunk 1/3 with 100 features...

ğŸ¯ Running Feature Importance for Classification
   ğŸ“Š Features available: 100
   ğŸ¯ Features to select: 30
   ğŸ“‚ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   ğŸ‘¤ User ID: automl_user
   ğŸ·ï¸ Model literal: automl_model
   ğŸ¯ Target column: target
ğŸ“Š Will rank all 100 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:43:41 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:43:41 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ§  Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:43:42 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[HOUSE_MDU_IND,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,HAS_DATA_IND,HAS_VIDEO_IND,HAS_VOICE_IND,HAS_MOBILE_IND,FTTH_IND,BAC_IND,ACP_IND,OPTIMUM_COMPLETE_IND,PREMIUM_HBO_IND,PREMIUM_MAX_IND,STREAM_IND,FIXED_SUB_AUTO_PAY_IND,PIA_PAID_IND,CARE_FIBER_OFFER_ELIGIBLE_IND,RETENTION_FIBER_OFFER_ELIGIBLE_IND,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,SPEED_ADDED_PREV_30D_IND,SPEED_ADDED_PREV_60D_IND,SPEED_ADDED_PREV_90D_IND,VIDEO_TIERS_ADDED_PREV_30D_IND,VIDEO_TIERS_ADDED_PREV_60D_IND,VIDEO_TIERS_ADDED_PREV_90D_IND,UPGRADE_IND_PREV_30D_IND,UPGRADE_IND_PREV_60D_IND,UPGRADE_IND_PREV_90D_IND,DATA_PSU_UPGRADES_PREV_30D_IND,DATA_PSU_UPGRADES_PREV_60D_IND,DATA_PSU_UPGRADES_PREV_90D_IND,VIDEO_PSU_UPGRADES_PREV_30D_IND,VIDEO_PSU_UPGRADES_PREV_60D_IND,VIDEO_PSU_UPGRADES_PREV_90D_IND,VOICE_PSU_UPGRADES_PREV_30D_IND,VOICE_PSU_UPGRADES_PREV_60D_IND,VOICE_PSU_UPGRADES_PREV_90D_IND,DOWNGRADE_IND_PREV_30D_IND,DOWNGRADE_IND_PREV_60D_IND,DOWNGRADE_IND_PREV_90D_IND,DATA_PSU_DOWNGRADES_PREV_30D_IND,DATA_PSU_DOWNGRADES_PREV_60D_IND,DATA_PSU_DOWNGRADES_PREV_90D_IND,VIDEO_PSU_DOWNGRADES_PREV_30D_IND,VIDEO_PSU_DOWNGRADES_PREV_60D_IND,VIDEO_PSU_DOWNGRADES_PREV_90D_IND,VOICE_PSU_DOWNGRADES_PREV_30D_IND,VOICE_PSU_DOWNGRADES_PREV_60D_IND,VOICE_PSU_DOWNGRADES_PREV_90D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_VOD_MINS_M0,USAGE_VIDEO_STB_DVR_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,USAGE_VIDEO_STB_VOD_MINS_M1,USAGE_VIDEO_STB_DVR_MINS_M1,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_30D,OVERDUE_BALANCE_60D,OVERDUE_BALANCE_90D,OVERDUE_BALANCE_120D,OVERDUE_BALANCE_OVR,CNT_COLLECTIONS_HARD_DISCONNECT_PAST30D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST60D,CNT_COLLECTIONS_HARD_DISCONNECT_PAST90D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST30D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST60D,CNT_COLLECTIONS_PAYMENT_PLAN_PAST90D,DAYS_COLLECTIONS_PENDING_NPD_PAST30D,DAYS_COLLECTIONS_PENDING_NPD_PAST60D,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,DAYS_IN_COLLECTIONS_PAST30D,DAYS_IN_COLLECTIONS_PAST60D,DAYS_IN_COLLECTIONS_PAST90D,WO_RATE_EVENT_IND_PREV_60D_CNT,WO_RATE_EVENT_IND_PREV_180D_CNT,WO_PROMO_ROLLOFF_IND_PREV_60D_CNT,WO_PROMO_ROLLOFF_IND_PREV_180D_CNT,CALLS_AGENT_OFFERED_PREV_7D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT],|filters=[]
25/09/24 21:43:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHI0THV3LVRoMGJIMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:42 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHI0THV3LVRoMGJIMhoCamkaAmpm
25/09/24 21:43:42 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:43:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEhUbmxrbXN2S2MzMhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:43:43 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEhUbmxrbXN2S2MzMhoCamkaAmpm
âœ… Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
âœ… Successfully selected 30 features using Random Forest
Selected 30 features from chunk 1
Cleaned up chunk data, pausing briefly...

Processing chunk 2/3 with 100 features...

ğŸ¯ Running Feature Importance for Classification
   ğŸ“Š Features available: 100
   ğŸ¯ Features to select: 30
   ğŸ“‚ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   ğŸ‘¤ User ID: automl_user
   ğŸ·ï¸ Model literal: automl_model
   ğŸ¯ Target column: target
ğŸ“Š Will rank all 100 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:02 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:02 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:02 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:02 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:03 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:03 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ§  Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:03 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[OMS_AREA,OMS_REGION,OMS_MARKET,HOUSE_CLEANSED_STATE,DWELL_CODE,DEMOS_AGE_RANGE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_SPENDING_TYPE_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_ACCOUNT_STATUS,FIXED_ACTIVE_RESI_BUS_DESC,FIXED_CUSTOMER_EBILL_STATUS,BOX_EQUIP_CLASS,BOX_EQUIP_CLASS_GROUP,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_OFFERED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_7D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_30D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_7D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_30D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_SALES_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_PREV_30D_UNIQUE_CNT,IVR_FIXED_TOTAL_CNT_PREV_7D_CNT,IVR_CSR_FIXED_CNT_PREV_7D_CNT,IVR_TSR_FIXED_CNT_PREV_7D_CNT,IVR_SALES_FIXED_CNT_PREV_7D_CNT,IVR_RETENTION_FIXED_CNT_PREV_7D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_CSR_FIXED_CNT_PREV_30D_CNT,IVR_TSR_FIXED_CNT_PREV_30D_CNT,IVR_SALES_FIXED_CNT_PREV_30D_CNT,IVR_RETENTION_FIXED_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_60D_CNT,IVR_TSR_FIXED_CNT_PREV_60D_CNT,IVR_SALES_FIXED_CNT_PREV_60D_CNT,IVR_RETENTION_FIXED_CNT_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_90D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,IVR_SALES_FIXED_CNT_PREV_90D_CNT,IVR_RETENTION_FIXED_CNT_PREV_90D_CNT,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_UPLOAD_SPEED_LOWEST,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_VENDOR_CNT,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FIBER_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FIBER_UPLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_HIGHEST,HOUSE_COMP_FW_UPLOAD_SPEED_LOWEST,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_DIVISION],|filters=[]
25/09/24 21:44:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHZnYTY0NnhfbGYzahoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHZnYTY0NnhfbGYzahoCamkaAmpm
25/09/24 21:44:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHhEZ1A4TTFNTU5pahoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:04 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHhEZ1A4TTFNTU5pahoCamkaAmpm
âœ… Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
âœ… Successfully selected 30 features using Random Forest
Selected 30 features from chunk 2
Cleaned up chunk data, pausing briefly...

Processing chunk 3/3 with 12 features...

ğŸ¯ Running Feature Importance for Classification
   ğŸ“Š Features available: 12
   ğŸ¯ Features to select: 30
   ğŸ“‚ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   ğŸ‘¤ User ID: automl_user
   ğŸ·ï¸ Model literal: automl_model
   ğŸ¯ Target column: target
ğŸ“Š Will rank all 12 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:21 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:21 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:22 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ§  Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,BILLING_COLLECTION_STATUS_DESC,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0],|filters=[]
25/09/24 21:44:22 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFZpNEFwRVZuM3BLRRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:22 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFZpNEFwRVZuM3BLRRoCamkaAmpm
25/09/24 21:44:22 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:23 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFDMTZiX3NpbmQycBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:23 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFDMTZiX3NpbmQycBoCamkaAmpm
âœ… Cached 100,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
âœ… Successfully selected 12 features using Random Forest
Selected 12 features from chunk 3

Combined 72 features from all chunks
Running final feature selection to select top 50 from 72 features...
Processing 72 features in chunks of 100
Created 1 chunks

Processing chunk 1/1 with 72 features...

ğŸ¯ Running Feature Importance for Classification
   ğŸ“Š Features available: 72
   ğŸ¯ Features to select: 30
   ğŸ“‚ Output directory: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
   ğŸ‘¤ User ID: automl_user
   ğŸ·ï¸ Model literal: automl_model
   ğŸ¯ Target column: target
ğŸ“Š Will rank all 72 features and select top 30
Attempt 1/3: Running Random Forest feature importance...
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:35 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
ğŸ§  Medium dataset (100,000 rows): Using MEMORY_ONLY storage
25/09/24 21:44:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_CREDIT_LINE_TYPE_DESC,DEMOS_CREDIT_USAGE_TYPE_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,FIXED_SUB_AUTO_PAY_IND,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_DATA_DOWNLOAD_SPEED,FIXED_VIDEO_TIER_DESC,FIXED_VOICE_PRICEPOINT_TIER_DESC,SPEED_ADDED_PREV_30D_IND,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M0,USAGE_VIDEO_STB_INTL_MINS_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M0,BILLING_VIDEO_OFFER_DESC_M0,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M0,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M1,BILLING_VIDEO_OFFER_DESC_M1,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M1,BILLING_FIXED_DATA_OFFER_GROUP2_DESC_M2,BILLING_VIDEO_OFFER_DESC_M2,BILLING_FIXED_VOICE_OFFER_GROUP2_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,VOICE_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,TOTAL_DATA_RECURRING_REVENUE_M1_M5_MAX,VIDEO_RECURRING_REVENUE_M1_M5_MAX,VOICE_RECURRING_REVENUE_M1_M5_MAX,BILLING_COLLECTION_STATUS_DESC,OVERDUE_BALANCE_OVR,DAYS_COLLECTIONS_PENDING_NPD_PAST90D,CALLS_AGENT_OFFERED_PREV_30D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_60D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_90D_TOTAL_CNT,CALLS_AGENT_OFFERED_PREV_30D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_60D_UNIQUE_CNT,CALLS_AGENT_OFFERED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_30D_CNT,CALLS_AGENT_OFFERED_CSR_FIXED_PREV_60D_CNT,CALLS_AGENT_HANDLED_PREV_60D_TOTAL_CNT,CALLS_AGENT_HANDLED_PREV_90D_UNIQUE_CNT,CALLS_AGENT_HANDLED_CSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_TSR_FIXED_PREV_90D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,IVR_FIXED_TOTAL_CNT_PREV_60D_CNT,IVR_CSR_FIXED_CNT_PREV_90D_CNT,IVR_TSR_FIXED_CNT_PREV_90D_CNT,HOUSE_COPPER_COMP,HOUSE_FIBER_COMP,HOUSE_COMP_VENDOR_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_HIGHEST,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_VENDOR_CNT,HOUSE_COMP_FW_DOWNLOAD_SPEED_LOWEST,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFZc0NVUzhDNHJvOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFZc0NVUzhDNHJvOBoCamkaAmpm
25/09/24 21:44:36 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:37 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDdzcUdHOHVxYk42cBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:37 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDdzcUdHOHVxYk42cBoCamkaAmpm
âœ… Cached 50,000 rows for feature importance calculation
Using RandomForest with 10 trees, max depth 10
âœ… Successfully selected 30 features using Random Forest
Selected 30 features from chunk 1

Combined 30 features from all chunks
Combined features (30) <= max_features (50), using all
âœ… Feature selection completed!

ğŸ“Š Generating final feature importance plot for 30 selected features...
25/09/24 21:44:48 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZLUzhJZmpWdUxvVBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDZLUzhJZmpWdUxvVBoCamkaAmpm
25/09/24 21:44:49 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHFEYWRmQ3g1TkZubxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHFEYWRmQ3g1TkZubxoCamkaAmpm
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDNUNUxQanlwaGhTdxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:50 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDNUNUxQanlwaGhTdxoCamkaAmpm
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:44:50 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:44:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEw1eHBSMHZrTGpSMBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDEw1eHBSMHZrTGpSMBoCamkaAmpm
25/09/24 21:44:51 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFc2UUptbElnd2JGcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:51 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFc2UUptbElnd2JGcxoCamkaAmpm
25/09/24 21:44:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGgtaXlUcVMtVlZLcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGgtaXlUcVMtVlZLcxoCamkaAmpm
25/09/24 21:44:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:44:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHBrbWdmQW1lZEJpcxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHBrbWdmQW1lZEJpcxoCamkaAmpm
25/09/24 21:44:53 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:44:53 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDFBNFNoQXhoYzJ5NhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:44:53 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDFBNFNoQXhoYzJ5NhoCamkaAmpm
25/09/24 21:45:04 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDE2LTl5ODhWMVhtZRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDDE2LTl5ODhWMVhtZRoCamkaAmpm
25/09/24 21:45:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHRCcGR3UTJ5WVQ2QxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:05 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHRCcGR3UTJ5WVQ2QxoCamkaAmpm
25/09/24 21:45:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHRvU2M5dFpiZS1LSxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDHRvU2M5dFpiZS1LSxoCamkaAmpm
25/09/24 21:45:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGl6TVVyb0xrRVNmUBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:06 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDGl6TVVyb0xrRVNmUBoCamkaAmpm
25/09/24 21:45:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFVIRTV0VE5hZEJ1ORoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDFVIRTV0VE5hZEJ1ORoCamkaAmpm
25/09/24 21:45:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,DEMOS_LIFESTAGE_DESC,DEMOS_ARCHETYPE_DESC,FIXED_TENURE_DAYS,FIXED_DATA_TENURE_DAYS,BOX_EQUIP_CLASS,FIXED_DATA_PRICEPOINT_TIER_DESC,FIXED_DATA_UPLOAD_SPEED,FIXED_VIDEO_TIER_DESC,USAGE_TOTAL_DOWN_BYTES_M0,USAGE_TOTAL_UP_BYTES_M0,USAGE_TOTAL_DOWN_BYTES_M1,USAGE_TOTAL_UP_BYTES_M1,USAGE_VIDEO_STB_NO_VOD_MINS_M1,BILLING_VIDEO_OFFER_DESC_M0,BILLING_VIDEO_OFFER_DESC_M2,TOTAL_RECURRING_REVENUE,TOTAL_DATA_RECURRING_REVENUE,VIDEO_RECURRING_REVENUE,TOTAL_RECURRING_REVENUE_M1_M5_MAX,OVERDUE_BALANCE_OVR,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,HOUSE_COMP_UPLOAD_SPEED_AVG,HOUSE_COMP_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OMS_MARKET],|filters=[]
25/09/24 21:45:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDF9COW55SV80UGp1dRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc': projects/atus-prism-dev/locations/us/sessions/CAISDF9COW55SV80UGp1dRoCamkaAmpm
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
âœ… Feature importance saved to Excel: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx
âœ… Enhanced feature importance plot saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/Features_selected_for_modeling.png
âœ… Feature importance Excel saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx
âœ… Final feature importance saved:
   ğŸ“Š Excel file: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/feature_importance.xlsx (exists: True)
   ğŸ“ˆ Plot file: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/Features_selected_for_modeling.png (exists: True)
Selected 30 features out of 212

ğŸ” Checking if full dataset switch is needed...
   Has _original_table_reference: True
   Has _data_manager: True

2.5. ğŸ”„ Switching to Full Dataset for Model Training...
25/09/24 21:45:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:09 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:45:09 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:09 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
25/09/24 21:45:10 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:10 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
   ğŸ“Š Current dataset size: 100,000 rows
   ğŸ¯ Selected features: 30 features
   ğŸ”„ Loading full dataset with selected features...
ğŸ”„ Switching to full dataset for model training...
ğŸ“Š Creating optimized table with 30 selected features
ğŸ”„ Mapped 30 encoded features to raw names:
   Example: OMS_MARKET_encoded â†’ OMS_MARKET
ğŸ“Š Creating optimized temp table from original atus-prism-dev.ds_sandbox.sundar_east_b2c_fixed_churn_train_prism
ğŸ” No filters specified - using full table
ğŸ“‹ Selecting 31 columns: ['target', 'OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0']...
ğŸ”§ Creating optimized temp table: atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852
âœ… Optimized temp table created: atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852 (expires in 24h)
25/09/24 21:45:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:14 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:14 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:14 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
âœ… Full dataset loaded: 2,596,745 rows Ã— 31 columns
ğŸ¯ Optimized: Full rows with selected features only
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:15 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   ğŸ“Š Full dataset loaded: 2,596,745 rows
   ğŸ“Š Full dataset columns: ['target', 'OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'BILLING_VIDEO_OFFER_DESC_M2', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'BILLING_VIDEO_OFFER_DESC_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS']
   ğŸ§¹ Clearing cached data before full dataset processing...
   ğŸ”„ Reprocessing full dataset with same preprocessing...
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:15 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   ğŸ“Š Full data before preprocessing: 2,596,745 rows
Starting data preprocessing...
ğŸ“Š Using full dataset (sample_fraction = 1.0)
ğŸ” Analyzing target column classes...
25/09/24 21:45:15 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:45:15 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEZzeDkzN1VadmZNNhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:15 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEZzeDkzN1VadmZNNhoCamkaAmpm
ğŸ“Š Detected 2 classes using distinct count
Initial feature count: 30
Features after date column filtering: 30
25/09/24 21:45:18 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:18 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:18 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,BILLING_VIDEO_OFFER_DESC_M0,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:18 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 31 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEFPajBicU8xaF9yUhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:18 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEFPajBicU8xaF9yUhoCamkaAmpm
Features after missing value filtering: 30
['OMS_MARKET', 'DEMOS_LIFESTAGE_DESC', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'BILLING_VIDEO_OFFER_DESC_M2', 'FIXED_DATA_PRICEPOINT_TIER_DESC', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'BILLING_VIDEO_OFFER_DESC_M0', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS']
Categorical variables: 10
Numerical variables: 20
ğŸ” Analyzing cardinality for 10 categorical variables...
ğŸ“Š Computing cardinality and missing values for all 10 variables in batch operations...
25/09/24 21:45:20 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,FIXED_VIDEO_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M0,DEMOS_ARCHETYPE_DESC,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:21 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 13 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDER2eElxTlItT09wRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:21 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDER2eElxTlItT09wRxoCamkaAmpm
25/09/24 21:45:21 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB
25/09/24 21:45:32 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB
25/09/24 21:45:35 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET,DEMOS_LIFESTAGE_DESC,DWELL_CODE,DEMOS_ECOHORT_GROUP_DESC,FIXED_VIDEO_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M2,FIXED_DATA_PRICEPOINT_TIER_DESC,BILLING_VIDEO_OFFER_DESC_M0,DEMOS_ARCHETYPE_DESC,BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 13 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDG9nZW5FOTJwMmRwchoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:36 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDG9nZW5FOTJwMmRwchoCamkaAmpm
25/09/24 21:45:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:43 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   ğŸ“Š OMS_MARKET: ~47 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_LIFESTAGE_DESC: ~49 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DWELL_CODE: ~36 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š DEMOS_ECOHORT_GROUP_DESC: ~17 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š FIXED_VIDEO_TIER_DESC: ~25 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_VIDEO_OFFER_DESC_M2: ~108 unique values â†’ DROPPING (exceeds threshold of 50)
   ğŸ“Š FIXED_DATA_PRICEPOINT_TIER_DESC: ~37 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BILLING_VIDEO_OFFER_DESC_M0: ~100 unique values â†’ DROPPING (exceeds threshold of 50)
   ğŸ“Š DEMOS_ARCHETYPE_DESC: ~9 unique values â†’ Keeping as CATEGORICAL
   ğŸ“Š BOX_EQUIP_CLASS: ~4 unique values â†’ Keeping as CATEGORICAL
ğŸ—‘ï¸ Dropping 2 high-cardinality categorical variables
âœ… Keeping 8 low-cardinality categorical variables
ğŸ—‘ï¸ Removing 2 high-cardinality categorical variables...
ğŸ’¡ These variables have >50 unique values and would create too many features after encoding
   Dropping columns: BILLING_VIDEO_OFFER_DESC_M2, BILLING_VIDEO_OFFER_DESC_M0
âœ… High-cardinality categorical variables removed from dataset
ğŸ“Š This prevents curse of dimensionality and improves model performance
Updated categorical variables: 8
Numerical variables unchanged: 20
ğŸ”„ Converting string columns to numeric where needed...
â™»ï¸ Reusing cardinality stats for 8 categorical features...
âœ… Using pre-calculated cardinality statistics (threshold: 200 unique values)
   â™»ï¸ OMS_MARKET: 47 unique values (reused)
   â™»ï¸ DEMOS_LIFESTAGE_DESC: 49 unique values (reused)
   â™»ï¸ DWELL_CODE: 36 unique values (reused)
   â™»ï¸ DEMOS_ECOHORT_GROUP_DESC: 17 unique values (reused)
   â™»ï¸ FIXED_VIDEO_TIER_DESC: 25 unique values (reused)
   â™»ï¸ FIXED_DATA_PRICEPOINT_TIER_DESC: 37 unique values (reused)
   â™»ï¸ DEMOS_ARCHETYPE_DESC: 9 unique values (reused)
   â™»ï¸ BOX_EQUIP_CLASS: 4 unique values (reused)
ğŸ“ˆ Categorical feature filtering: 8 â†’ 8 features (optimized)
25/09/24 21:45:43 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[OMS_MARKET],|filters=[]
25/09/24 21:45:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDIzdjgxeUR2V1QtThoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:44 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDDIzdjgxeUR2V1QtThoCamkaAmpm
25/09/24 21:45:44 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC],|filters=[]
25/09/24 21:45:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGkwV3RjcEVJRTdERBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:45 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGkwV3RjcEVJRTdERBoCamkaAmpm
25/09/24 21:45:46 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DWELL_CODE],|filters=[]
25/09/24 21:45:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGJKdVRVUEIxMkNrOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:46 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGJKdVRVUEIxMkNrOBoCamkaAmpm
25/09/24 21:45:47 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_ECOHORT_GROUP_DESC],|filters=[]
25/09/24 21:45:48 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 3 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGV3Tnpod25ybU1jbxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:48 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGV3Tnpod25ybU1jbxoCamkaAmpm
25/09/24 21:45:49 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[FIXED_VIDEO_TIER_DESC],|filters=[]
25/09/24 21:45:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFo5d053aEg3ZWIxbRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:49 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDFo5d053aEg3ZWIxbRoCamkaAmpm
25/09/24 21:45:50 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[FIXED_DATA_PRICEPOINT_TIER_DESC],|filters=[]
25/09/24 21:45:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDDZ6YlRpT2MwSmRyTRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:50 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDDZ6YlRpT2MwSmRyTRoCamkaAmpm
25/09/24 21:45:52 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_ARCHETYPE_DESC],|filters=[]
25/09/24 21:45:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 2 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHJOYjl0TDVOTW91RhoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:52 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDHJOYjl0TDVOTW91RhoCamkaAmpm
25/09/24 21:45:53 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[BOX_EQUIP_CLASS],|filters=[]
25/09/24 21:45:54 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDHlvMFlzV3R2c2JUSRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:45:54 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDHlvMFlzV3R2c2JUSRoCamkaAmpm
Categorical encoding step complete.
Numerical impuation step complete.
Categorical encoding complete with _encoded suffix.
Target and Preprocessed features joined.
Final Data Summary - 
âš ï¸ Spark session recovery utilities not available, using basic error handling
25/09/24 21:45:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:55 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:55 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:55 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:57 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:57 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
ğŸ“‹ Cached schema with 29 columns for recovery
{'row_count': 2596745, 'column_count': 29, 'columns': ['FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'TOTAL_RECURRING_REVENUE', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'DWELL_CODE_encoded', 'DEMOS_ECOHORT_GROUP_DESC_encoded', 'FIXED_VIDEO_TIER_DESC_encoded', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded', 'DEMOS_ARCHETYPE_DESC_encoded', 'BOX_EQUIP_CLASS_encoded', 'target'], 'dtypes': {'FIXED_TENURE_DAYS': 'bigint', 'USAGE_TOTAL_UP_BYTES_M0': 'decimal(38,9)', 'TOTAL_RECURRING_REVENUE': 'decimal(38,9)', 'FIXED_DATA_TENURE_DAYS': 'bigint', 'TOTAL_DATA_RECURRING_REVENUE': 'decimal(38,9)', 'USAGE_TOTAL_DOWN_BYTES_M1': 'decimal(38,9)', 'HOUSE_COMP_UPLOAD_SPEED_AVG': 'double', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT': 'decimal(38,9)', 'VIDEO_RECURRING_REVENUE': 'decimal(38,9)', 'USAGE_TOTAL_DOWN_BYTES_M0': 'decimal(38,9)', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG': 'double', 'USAGE_TOTAL_UP_BYTES_M1': 'decimal(38,9)', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG': 'double', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT': 'decimal(38,9)', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG': 'double', 'OVERDUE_BALANCE_OVR': 'decimal(38,9)', 'FIXED_DATA_UPLOAD_SPEED': 'decimal(38,9)', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1': 'bigint', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT': 'decimal(38,9)', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX': 'decimal(38,9)', 'OMS_MARKET_encoded': 'double', 'DEMOS_LIFESTAGE_DESC_encoded': 'double', 'DWELL_CODE_encoded': 'double', 'DEMOS_ECOHORT_GROUP_DESC_encoded': 'double', 'FIXED_VIDEO_TIER_DESC_encoded': 'double', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded': 'double', 'DEMOS_ARCHETYPE_DESC_encoded': 'double', 'BOX_EQUIP_CLASS_encoded': 'double', 'target': 'bigint'}}
Data preprocessing completed.
25/09/24 21:45:58 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:58 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:45:58 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:45:58 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
25/09/24 21:46:00 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:46:00 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 2596745
   ğŸ“Š Full data after preprocessing: 2,596,745 rows
   ğŸ’¾ Caching full_processed_data to prevent lazy evaluation issues...
25/09/24 21:46:01 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS,OMS_MARKET],|filters=[]
25/09/24 21:46:01 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 27 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEVfRW1lT3BaU1FBOBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:01 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEVfRW1lT3BaU1FBOBoCamkaAmpm
25/09/24 21:46:01 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:46:01 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDGZkMHB1QWxxOXRWRxoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:01 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDGZkMHB1QWxxOXRWRxoCamkaAmpm
   âœ… Full processed data cached and materialized
   âš ï¸ Feature BILLING_VIDEO_OFFER_DESC_M2_encoded not found in processed data (neither original nor encoded)
   âš ï¸ Feature BILLING_VIDEO_OFFER_DESC_M0_encoded not found in processed data (neither original nor encoded)
   ğŸ”„ Selecting 28 encoded features + target from processed data
   ğŸ“‹ Required columns: ['OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE_encoded']...
   ğŸ” DEBUG: full_processed_data before select: 76,251 rows
   ğŸ” DEBUG: Checking if all required columns exist in processed data...
   âœ… All 29 required columns found in processed data
   ğŸ” DEBUG: full_processed_data_final after select: 76,251 rows
   ğŸ”„ Created full_processed_data_final with 29 columns
   ğŸ”„ Switched processed_data from sampled to full dataset
   ğŸ”„ Updated selected_vars to use encoded feature names: 28 features
   ğŸ’¾ Cached final processed_data for efficient reuse
   ğŸ§¹ Cleaning up intermediate full_processed_data cache...
25/09/24 21:46:06 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[DEMOS_LIFESTAGE_DESC,FIXED_TENURE_DAYS,USAGE_TOTAL_UP_BYTES_M0,DWELL_CODE,TOTAL_RECURRING_REVENUE,DEMOS_ECOHORT_GROUP_DESC,FIXED_DATA_TENURE_DAYS,TOTAL_DATA_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M1,HOUSE_COMP_UPLOAD_SPEED_AVG,FIXED_VIDEO_TIER_DESC,IVR_FIXED_TOTAL_CNT_PREV_30D_CNT,VIDEO_RECURRING_REVENUE,USAGE_TOTAL_DOWN_BYTES_M0,HOUSE_COMP_DOWNLOAD_SPEED_AVG,FIXED_DATA_PRICEPOINT_TIER_DESC,USAGE_TOTAL_UP_BYTES_M1,HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT,HOUSE_COMP_FW_UPLOAD_SPEED_AVG,OVERDUE_BALANCE_OVR,FIXED_DATA_UPLOAD_SPEED,USAGE_VIDEO_STB_NO_VOD_MINS_M1,CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT,DEMOS_ARCHETYPE_DESC,TOTAL_RECURRING_REVENUE_M1_M5_MAX,BOX_EQUIP_CLASS,OMS_MARKET],|filters=[]
25/09/24 21:46:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 27 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDEpZYV9jSzBxeDNBZBoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDEpZYV9jSzBxeDNBZBoCamkaAmpm
25/09/24 21:46:07 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852, parameters sent from Spark:|requiredColumns=[target],|filters=[]
25/09/24 21:46:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/atus-prism-dev/locations/us/sessions/CAISDFB5NC1VelU2Z256LRoCamkaAmpm. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
25/09/24 21:46:07 INFO BigQueryRDDFactory: Created read session for table 'atus-prism-dev.ds_sandbox.automl_temp_optimized_1758750310_bfe8d852': projects/atus-prism-dev/locations/us/sessions/CAISDFB5NC1VelU2Z256LRoCamkaAmpm
   âœ… Successfully switched to full dataset: 76,251 rows
Final selected variables: ['OMS_MARKET_encoded', 'DEMOS_LIFESTAGE_DESC_encoded', 'FIXED_TENURE_DAYS', 'USAGE_TOTAL_UP_BYTES_M0', 'DWELL_CODE_encoded', 'TOTAL_RECURRING_REVENUE', 'DEMOS_ECOHORT_GROUP_DESC_encoded', 'FIXED_DATA_TENURE_DAYS', 'TOTAL_DATA_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M1', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'FIXED_VIDEO_TIER_DESC_encoded', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'VIDEO_RECURRING_REVENUE', 'USAGE_TOTAL_DOWN_BYTES_M0', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'FIXED_DATA_PRICEPOINT_TIER_DESC_encoded', 'USAGE_TOTAL_UP_BYTES_M1', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG', 'OVERDUE_BALANCE_OVR', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'DEMOS_ARCHETYPE_DESC_encoded', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'BOX_EQUIP_CLASS_encoded']
Final categorical variables: []
Final numerical variables: ['FIXED_TENURE_DAYS', 'FIXED_DATA_TENURE_DAYS', 'FIXED_DATA_UPLOAD_SPEED', 'USAGE_TOTAL_DOWN_BYTES_M0', 'USAGE_TOTAL_UP_BYTES_M0', 'USAGE_TOTAL_DOWN_BYTES_M1', 'USAGE_TOTAL_UP_BYTES_M1', 'USAGE_VIDEO_STB_NO_VOD_MINS_M1', 'TOTAL_RECURRING_REVENUE', 'TOTAL_DATA_RECURRING_REVENUE', 'VIDEO_RECURRING_REVENUE', 'TOTAL_RECURRING_REVENUE_M1_M5_MAX', 'OVERDUE_BALANCE_OVR', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT', 'CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT', 'IVR_FIXED_TOTAL_CNT_PREV_30D_CNT', 'HOUSE_COMP_UPLOAD_SPEED_AVG', 'HOUSE_COMP_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG', 'HOUSE_COMP_FW_UPLOAD_SPEED_AVG']

3. Data Splitting and Scaling...
ğŸ” VERIFICATION: processed_data contains 76,251 rows before split_and_scale
ğŸ” VERIFICATION: data contains 76,251 rows before split_and_scale
Splitting data into train/validation/test sets...

ğŸ“Š Analyzing class distribution on full dataset...
ğŸ” VERIFICATION: data contains 76,251 rows before analyze_class_distribution
Analyzing class distribution for target...
ğŸ” VERIFICATION: data contains 76,251 rows before analyze_class_distribution
ğŸ” VERIFICATION: total_count contains 76,251 rows after analyze_class_distribution
ğŸ” VERIFICATION: Sum of class counts: 76,251 rows
âœ… Class count verification passed: 76,251 = 76,251
Class distribution:
  Class 1: 771 samples (1.01%)
  Class 0: 75480 samples (98.99%)
âš ï¸  Imbalanced dataset detected: 1 class(es) below 5.0% threshold
    - Class 1: 1.01% (< 5.0%)
ğŸ” VERIFICATION: data contains 76,251 rows after analyze_class_distribution
ğŸ” VERIFICATION: data contains 76,251 rows after train_valid_test_split
ğŸ” VERIFICATION: train contains 45,779 rows after train_valid_test_split
ğŸ” VERIFICATION: valid contains 15,029 rows after train_valid_test_split
ğŸ” VERIFICATION: test contains 15,443 rows after train_valid_test_split

ğŸ“ˆ Applying oversample to training data...
Upsampling minority classes using oversample method...
Target minority class size: 3774 samples (5.0% of majority class)
  Upsampling class 1: 771 â†’ 3774 (+3003 samples)
Upsampling completed. New dataset size: 47670 samples

ğŸ“Š Post-upsampling class distribution:
Analyzing class distribution for target...
ğŸ” VERIFICATION: data contains 47,670 rows before analyze_class_distribution
ğŸ” VERIFICATION: total_count contains 47,670 rows after analyze_class_distribution
ğŸ” VERIFICATION: Sum of class counts: 47,670 rows
âœ… Class count verification passed: 47,670 = 47,670
Class distribution:
  Class 1: 2366 samples (4.96%)
  Class 0: 45304 samples (95.04%)
âœ… Training data upsampling completed. Validation/test data remain original.
ğŸ“‹ Note: Model will train on upsampled data, but training evaluation will use original data.
Data splitting and scaling completed.
ğŸ” VERIFICATION: Train_scaled contains 47,670 rows after split_and_scale
ğŸ” VERIFICATION: Train_original_scaled contains 45,779 rows after split_and_scale
ğŸ” VERIFICATION: valid_scaled contains 15,029 rows after split_and_scale
ğŸ” VERIFICATION: test_scaled contains 15,443 rows after split_and_scale

4. Preparing Out-of-Time Datasets...

ğŸš€ Optimizing OOT datasets to use only selected features...
ğŸ“‹ Required columns for OOT datasets: 29 columns
   Features: 28, Target: 1

4.5. Feature Profiling for All Datasets...
â™»ï¸ Reusing cardinality stats for 10 categorical features
ğŸ” Profiling 1 datasets with 30 selected features

ğŸ“Š Profiling train dataset...
â™»ï¸ Using existing cardinality stats for train dataset
ğŸ”„ Mapped 30 encoded features to 20 raw features for profiling
   Example: OMS_MARKET_encoded â†’ FIXED_TENURE_DAYS
ğŸ” Starting feature profiling for 20 selected features...
ğŸ“Š Performing combined univariate analysis and data quality assessment...
â™»ï¸ Reusing cardinality and missing value statistics...
   ğŸš€ Performing vectorized analysis for 20 features...
   ğŸ“Š Processing 20 numerical and 0 categorical features...
   ğŸ§® Computing all numerical statistics in single operation...
   ğŸ” Performing vectorized quality assessment...
     ğŸ” Calculating missing unique count for FIXED_TENURE_DAYS...
     âœ… FIXED_TENURE_DAYS: 13558 unique values calculated
     ğŸ” Calculating missing unique count for USAGE_TOTAL_UP_BYTES_M0...
     âœ… USAGE_TOTAL_UP_BYTES_M0: 68498 unique values calculated
     ğŸ” Calculating missing unique count for TOTAL_RECURRING_REVENUE...
     âœ… TOTAL_RECURRING_REVENUE: 5696 unique values calculated
     ğŸ” Calculating missing unique count for FIXED_DATA_TENURE_DAYS...
     âœ… FIXED_DATA_TENURE_DAYS: 7692 unique values calculated
     ğŸ” Calculating missing unique count for TOTAL_DATA_RECURRING_REVENUE...
     âœ… TOTAL_DATA_RECURRING_REVENUE: 1593 unique values calculated
     ğŸ” Calculating missing unique count for USAGE_TOTAL_DOWN_BYTES_M1...
     âœ… USAGE_TOTAL_DOWN_BYTES_M1: 68238 unique values calculated
     ğŸ” Calculating missing unique count for HOUSE_COMP_UPLOAD_SPEED_AVG...
     âœ… HOUSE_COMP_UPLOAD_SPEED_AVG: 1587 unique values calculated
     ğŸ” Calculating missing unique count for IVR_FIXED_TOTAL_CNT_PREV_30D_CNT...
     âœ… IVR_FIXED_TOTAL_CNT_PREV_30D_CNT: 40 unique values calculated
     ğŸ” Calculating missing unique count for VIDEO_RECURRING_REVENUE...
     âœ… VIDEO_RECURRING_REVENUE: 1844 unique values calculated
     ğŸ” Calculating missing unique count for USAGE_TOTAL_DOWN_BYTES_M0...
     âœ… USAGE_TOTAL_DOWN_BYTES_M0: 68630 unique values calculated
     ğŸ” Calculating missing unique count for HOUSE_COMP_DOWNLOAD_SPEED_AVG...
     âœ… HOUSE_COMP_DOWNLOAD_SPEED_AVG: 1685 unique values calculated
     ğŸ” Calculating missing unique count for USAGE_TOTAL_UP_BYTES_M1...
     âœ… USAGE_TOTAL_UP_BYTES_M1: 68209 unique values calculated
     ğŸ” Calculating missing unique count for HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG...
     âœ… HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG: 232 unique values calculated
     ğŸ” Calculating missing unique count for CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT...
     âœ… CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT: 8 unique values calculated
     ğŸ” Calculating missing unique count for HOUSE_COMP_FW_UPLOAD_SPEED_AVG...
     âœ… HOUSE_COMP_FW_UPLOAD_SPEED_AVG: 270 unique values calculated
     ğŸ” Calculating missing unique count for OVERDUE_BALANCE_OVR...
     âœ… OVERDUE_BALANCE_OVR: 5407 unique values calculated
     ğŸ” Calculating missing unique count for FIXED_DATA_UPLOAD_SPEED...
     âœ… FIXED_DATA_UPLOAD_SPEED: 17 unique values calculated
     ğŸ” Calculating missing unique count for USAGE_VIDEO_STB_NO_VOD_MINS_M1...
     âœ… USAGE_VIDEO_STB_NO_VOD_MINS_M1: 20083 unique values calculated
     ğŸ” Calculating missing unique count for CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT...
     âœ… CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT: 14 unique values calculated
     ğŸ” Calculating missing unique count for TOTAL_RECURRING_REVENUE_M1_M5_MAX...
     âœ… TOTAL_RECURRING_REVENUE_M1_M5_MAX: 5986 unique values calculated
âœ… Combined univariate analysis and Population Stability Index assessment completed
   â™»ï¸ Reused cardinality and missing values for ALL 20 features
   ğŸ“Š Overall stability grade: A (95.0/100)
   âš ï¸ Stability warnings found: 0
ğŸ“ˆ Performing bivariate analysis...
   ğŸ“ˆ Analyzing target distribution by FIXED_TENURE_DAYS...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by USAGE_TOTAL_UP_BYTES_M0...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by TOTAL_RECURRING_REVENUE...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by FIXED_DATA_TENURE_DAYS...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by TOTAL_DATA_RECURRING_REVENUE...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by USAGE_TOTAL_DOWN_BYTES_M1...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by HOUSE_COMP_UPLOAD_SPEED_AVG...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by IVR_FIXED_TOTAL_CNT_PREV_30D_CNT...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by VIDEO_RECURRING_REVENUE...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by USAGE_TOTAL_DOWN_BYTES_M0...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by HOUSE_COMP_DOWNLOAD_SPEED_AVG...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by USAGE_TOTAL_UP_BYTES_M1...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by HOUSE_COMP_FW_DOWNLOAD_SPEED_AVG...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_7D_CNT...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by HOUSE_COMP_FW_UPLOAD_SPEED_AVG...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by OVERDUE_BALANCE_OVR...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by FIXED_DATA_UPLOAD_SPEED...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by USAGE_VIDEO_STB_NO_VOD_MINS_M1...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by CALLS_AGENT_HANDLED_RETENTION_FIXED_PREV_60D_CNT...
     ğŸ“Š Processing as numerical feature...
   ğŸ“ˆ Analyzing target distribution by TOTAL_RECURRING_REVENUE_M1_M5_MAX...
     ğŸ“Š Processing as numerical feature...
ğŸ”— Computing correlation matrix...
   ğŸ”— Computing correlations for 20 features using optimized MLlib approach...
   âœ… Target column 'target' is already numeric
   ğŸ“Š Assembling feature vectors for 21 columns...
   ğŸ§® Computing pearson correlation matrix...
   ğŸ” Analyzing feature-feature correlations for multicollinearity...
   âœ… Correlation analysis complete: 27 high correlations found
âœ… Feature profiling completed for 20 features
âœ… train profiling completed and saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/automl_user_train_feature_profiling.json
âœ… Combined profiling results saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/automl_user_all_datasets_feature_profiling.json
ğŸ” Successfully profiled 1 datasets with 30 features
25/09/24 21:47:05 INFO DirectBigQueryRelation: |Querying table atus-prism-dev.ds_sandbox.automl_temp_sample_1758750009_9fb020cc, parameters sent from Spark:|requiredColumns=[],|filters=[]
25/09/24 21:47:05 INFO DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 100000
âœ… Cross-validation enabled: no out-of-time validation data available

5. Model Building and Validation...
ğŸ”„ Using cross-validation for model training and validation...
Cross-validation dataset size: 78142 samples
ğŸ“‹ Note: CV training uses upsampled data, but evaluation metrics use original data

ğŸ“Š Cross-Validation Metrics Configuration:
  Binary classification: areaUnderROC
  Multiclass classification: accuracy
  ğŸ“ˆ areaUnderROC: Measures ability to distinguish between classes
  ğŸ¯ accuracy: Overall classification accuracy
Building xgboost model with 5-fold cross-validation...

ğŸ”„ Training both default and tuned xgboost models with 5-fold CV...
âœ… Spark session is healthy
Using areaUnderROC as cross-validation metric for binary classification
  ğŸ“Š Training default xgboost model with CV...
Creating xgboost estimator for cross-validation...
âœ… XGBoost estimator created successfully with parameters: {}
   Features column: features
   Label column: target
xgboost estimator created successfully.
2025-09-24 21:47:11,501 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:47:22,503 INFO XGBoost-PySpark: _fit Finished xgboost training!
25/09/24 21:47:23 INFO BaseAllocator: Debug mode disabled.
25/09/24 21:47:23 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
25/09/24 21:47:23 INFO CheckAllocator: Using DefaultAllocationManager at memory-netty-11.0.0.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class
2025-09-24 21:47:44,455 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:47:48,623 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:03,967 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:08,115 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:23,385 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:27,488 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:48:42,021 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:logistic', 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:48:46,221 INFO XGBoost-PySpark: _fit Finished xgboost training!
2025-09-24 21:49:00,910 INFO XGBoost-PySpark: _fit Running xgboost-2.1.3 on 1 workers with
	booster params: {'objective': 'binary:logistic', 'colsample_bytree': 1.0, 'device': 'cpu', 'gamma': 0.0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1.0, 'eta': 0.3, 'nthread': 1}
	train_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}
	dmatrix_kwargs: {'nthread': 1, 'missing': nan}
2025-09-24 21:49:06,729 INFO XGBoost-PySpark: _fit Finished xgboost training!
Validating xgboost_default model on 3 datasets...
Validating on train dataset...
Model prediction completed
AUC calculated 0.9992806955584262
Accuracy calculated 0.9986675113043099
KS calculation starting
KS_Value = 90.73
Metrics calculation process Completed in :  9.896067142486572 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for train data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model train.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for train data.png
All validation metrics computed
Model validation process completed in: 10.80 seconds
Validating on valid dataset...
Model prediction completed
AUC calculated 0.9606439885978209
Accuracy calculated 0.9912169805043582
KS calculation starting
KS_Value = 76.73
Metrics calculation process Completed in :  5.641307592391968 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for valid data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model valid.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for valid data.png
All validation metrics computed
Model validation process completed in: 6.27 seconds
Validating on test dataset...
Model prediction completed
AUC calculated 0.9777210439615176
Accuracy calculated 0.9922942433465001
KS calculation starting
KS_Value = 83.31
Metrics calculation process Completed in :  5.532714605331421 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - ROC for test data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model test.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default Model - Confusion Matrix for test data.png
All validation metrics computed
Model validation process completed in: 6.17 seconds
Metrics saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_metrics.z and /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_metrics.json
Looking for KS files with pattern: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS xgboost_default Model*.xlsx
Found 3 KS files to process
Sheet name: 'KS xgboost_default Model valid' -> 'XGB_def_valid'
âœ“ Conditional formatting applied to XGB_def_valid
âœ“ Processed and marked for removal: KS xgboost_default Model valid.xlsx
Sheet name: 'KS xgboost_default Model test' -> 'XGB_def_test'
âœ“ Conditional formatting applied to XGB_def_test
âœ“ Processed and marked for removal: KS xgboost_default Model test.xlsx
Sheet name: 'KS xgboost_default Model train' -> 'XGB_def_train'
âœ“ Conditional formatting applied to XGB_def_train
âœ“ Processed and marked for removal: KS xgboost_default Model train.xlsx
âœ“ Removed: KS xgboost_default Model valid.xlsx
âœ“ Removed: KS xgboost_default Model test.xlsx
âœ“ Removed: KS xgboost_default Model train.xlsx
Total files processed: 3
KS charts consolidated in: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/KS_Charts.xlsx
Note: Conditional formatting should now be visible in the Excel file
Validation summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_default/xgboost_default_validation_summary.txt
xgboost_default model validation completed.
  ğŸ“ Hyperparameter optimization disabled - using default xgboost model
Model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/xgboost_model
âœ… xgboost CV model training completed - selected default version
Building lightgbm model with 5-fold cross-validation...

ğŸ”„ Training both default and tuned lightgbm models with 5-fold CV...
âœ… Spark session is healthy
Using areaUnderROC as cross-validation metric for binary classification
  ğŸ“Š Training default lightgbm model with CV...
Creating lightgbm estimator for cross-validation...
lightgbm estimator created successfully.
ğŸ”„ fitMultiple called for Native LightGBM cross-validation
ğŸ“Š Training model 1/1 for CV fold
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
ğŸ“Š Training data: 62512 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(60417), np.int64(1): np.int64(2095)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ”„ fitMultiple called for Native LightGBM cross-validation
ğŸ“Š Training model 1/1 for CV fold
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ“Š Training data: 62754 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(60612), np.int64(1): np.int64(2142)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ”„ fitMultiple called for Native LightGBM cross-validation
ğŸ“Š Training model 1/1 for CV fold
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ“Š Training data: 62544 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(60394), np.int64(1): np.int64(2150)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ”„ fitMultiple called for Native LightGBM cross-validation
ğŸ“Š Training model 1/1 for CV fold
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ“Š Training data: 62409 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(60301), np.int64(1): np.int64(2108)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ”„ fitMultiple called for Native LightGBM cross-validation
ğŸ“Š Training model 1/1 for CV fold
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ“Š Training data: 62337 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(60185), np.int64(1): np.int64(2152)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸš€ Training Native LightGBM with objective: binary
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
ğŸ“Š Training data: 78142 samples, 28 features
ğŸ¯ Number of classes: 2
ğŸ“‹ Class distribution: {np.int64(0): np.int64(75480), np.int64(1): np.int64(2662)}
ğŸ”§ LightGBM parameters: {'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 1.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'seed': 42, 'verbose': -1, 'force_row_wise': True}
ğŸ‹ï¸ Training LightGBM model...
âœ… Native LightGBM model trained successfully!
ğŸ“Š Model info: 100 trees, 28 features
Validating lightgbm_default model on 3 datasets...
Validating on train dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.993106127494261
Accuracy calculated 0.994407916293497
KS calculation starting
KS_Value = 88.82
Metrics calculation process Completed in :  10.623364448547363 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for train data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model train.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for train data.png
All validation metrics computed
Model validation process completed in: 29.26 seconds
Validating on valid dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.851038283899834
Accuracy calculated 0.9901523720806441
KS calculation starting
KS_Value = 53.07
Metrics calculation process Completed in :  6.853440284729004 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for valid data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model valid.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for valid data.png
All validation metrics computed
Model validation process completed in: 11.67 seconds
Validating on test dataset...
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
Model prediction completed
AUC calculated 0.8517588520763953
Accuracy calculated 0.9906106326490967
KS calculation starting
KS_Value = 51.43
Metrics calculation process Completed in :  4.860246896743774 seconds
ROC plot saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - ROC for test data.png
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
Decile table saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model test.xlsx
Confusion matrix saved: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default Model - Confusion Matrix for test data.png
All validation metrics computed
Model validation process completed in: 10.01 seconds
Metrics saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_metrics.z and /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_metrics.json
Looking for KS files with pattern: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS lightgbm_default Model*.xlsx
Found 3 KS files to process
Sheet name: 'KS lightgbm_default Model test' -> 'LGB_def_test'
âœ“ Conditional formatting applied to LGB_def_test
âœ“ Processed and marked for removal: KS lightgbm_default Model test.xlsx
Sheet name: 'KS lightgbm_default Model valid' -> 'LGB_def_valid'
âœ“ Conditional formatting applied to LGB_def_valid
âœ“ Processed and marked for removal: KS lightgbm_default Model valid.xlsx
Sheet name: 'KS lightgbm_default Model train' -> 'LGB_def_train'
âœ“ Conditional formatting applied to LGB_def_train
âœ“ Processed and marked for removal: KS lightgbm_default Model train.xlsx
âœ“ Removed: KS lightgbm_default Model test.xlsx
âœ“ Removed: KS lightgbm_default Model valid.xlsx
âœ“ Removed: KS lightgbm_default Model train.xlsx
Total files processed: 3
KS charts consolidated in: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/KS_Charts.xlsx
Note: Conditional formatting should now be visible in the Excel file
Validation summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_default/lightgbm_default_validation_summary.txt
lightgbm_default model validation completed.
  ğŸ“ Hyperparameter optimization disabled - using default lightgbm model
Model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
âœ… lightgbm CV model training completed - selected default version

ğŸ“Š Model Building Summary:
   âœ… Successfully built: xgboost, lightgbm
   âŒ Failed to build: None

6. Model Selection...
Selecting best model based on ks on valid dataset...
Debug: Created DataFrame with shape: (2, 12)
Debug: DataFrame columns: ['model_type', 'roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'cv_score', 'selection_info']
Debug: DataFrame head:
  model_type  ...                                     selection_info
0    xgboost  ...  {'decision': 'default', 'reasons': ['Hyperpara...
1   lightgbm  ...  {'decision': 'default', 'reasons': ['Hyperpara...

[2 rows x 12 columns]
Debug: best_model_info keys: ['model_type', 'selection_criteria', 'dataset_used', 'performance_score', 'stability_score', 'rank', 'all_results']
Debug: DataFrame columns: ['model_type', 'roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'cv_score', 'selection_info', 'counter', 'selected_model']
/var/dataproc/tmp/srvls-batch-f34b5be2-a020-4110-8ac5-b9ca2e276673/job_0002_automl_user_automl_model_1758749709_script.py:786: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.
  return original_to_excel(self, excel_writer, sheet_name, **kwargs)
âœ… Model selection results saved to Excel: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_results.xlsx
Selection results saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_results.xlsx
Selection summary saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_summary.txt
Best model selected: lightgbm
Loaded lightgbm model from /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
âœ… Best model loaded successfully: lightgbm
Best model selected: lightgbm

7. Generating Scoring Code...
Generating scoring scripts...
Generated xgboost_model_scoring.py
Generated lightgbm_model_scoring.py
Scoring scripts generated in /tmp/automl_results/job_0002_automl_user_automl_model_1758749709

8. Save model configuration files...
âœ… Model info saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_info.pkl
âœ… Character labels saved
âœ… Pipeline model saved
âœ… lightgbm model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/lightgbm_model
ğŸ‰ Complete model saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709
ğŸ“ Saved files:
   - model_info.pkl
   - lightgbm_model/
   - char_labels/
   - pipeline_model/

9. Computing SHAP values for best model (lightgbm) explainability...
   ğŸ“Š Using processed dataset for SHAP: 76,251 rows
   ğŸ¯ SHAP will analyze 28 selected features
   ğŸ“Š SHAP feature breakdown: 28 numerical, 0 categorical
25/09/24 21:52:54 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:55 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:55 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  0%|          | 0/50 [00:00<?, ?it/s]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:56 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  2%|â–         | 1/50 [00:03<02:36,  3.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:52:59 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:52:59 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  4%|â–         | 2/50 [00:05<02:11,  2.74s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:03 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:03 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  6%|â–Œ         | 3/50 [00:07<01:57,  2.51s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:04 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:04 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:05 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:05 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

  8%|â–Š         | 4/50 [00:10<01:54,  2.49s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:06 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:06 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:08 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 10%|â–ˆ         | 5/50 [00:12<01:49,  2.44s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:08 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:08 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:10 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 12%|â–ˆâ–        | 6/50 [00:14<01:43,  2.36s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:10 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:10 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:12 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:12 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 14%|â–ˆâ–        | 7/50 [00:17<01:40,  2.35s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:14 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:14 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 16%|â–ˆâ–Œ        | 8/50 [00:19<01:35,  2.27s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 18%|â–ˆâ–Š        | 9/50 [00:21<01:33,  2.28s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 20%|â–ˆâ–ˆ        | 10/50 [00:23<01:31,  2.29s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 22%|â–ˆâ–ˆâ–       | 11/50 [00:25<01:25,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:23 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:23 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 24%|â–ˆâ–ˆâ–       | 12/50 [00:28<01:23,  2.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:24 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:24 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:25 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:25 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:30<01:20,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:27 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:27 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:32<01:16,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:28 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:29 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:29 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:34<01:15,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:30 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:30 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:36<01:13,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:34 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:38<01:11,  2.17s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:34 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:34 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:36 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:36 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:40<01:09,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:37 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:37 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:38 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:38 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:43<01:06,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:39 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:39 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:40 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:45<01:05,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:41 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:41 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:47<01:03,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:43 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:43 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:49<01:00,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:47 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:51<00:59,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:47 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:47 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:48 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:49 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:49 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:54<00:57,  2.20s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:50 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:50 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:51 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:51 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:56<00:55,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:52 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:52 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:53 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:53 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:58<00:52,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:54 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:54 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:55 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:55 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:55 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:00<00:49,  2.15s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:56 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:56 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:53:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:02<00:47,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:53:58 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:53:58 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:00 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:00 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [01:04<00:45,  2.19s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:01 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:01 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:02 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:02 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:07<00:42,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:03 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:03 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:04 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:04 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:09<00:41,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:05 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:05 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:06 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:06 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:11<00:38,  2.16s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:07 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:07 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:09 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:13<00:37,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:09 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:09 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:11 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [01:15<00:34,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:11 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:11 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [01:17<00:31,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:13 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:13 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [01:19<00:29,  2.13s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:15 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:15 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:17 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:17 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [01:22<00:27,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:18 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:18 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:19 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:19 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [01:24<00:25,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:20 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:20 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:21 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:21 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [01:26<00:23,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:22 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:22 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:23 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:23 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:28<00:21,  2.11s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:24 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:24 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:30<00:19,  2.14s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:26 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:26 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:32<00:17,  2.21s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:28 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:28 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:29 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:30 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:30 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:35<00:15,  2.18s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:31 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:31 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:32 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:32 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:37<00:13,  2.24s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:33 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:33 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:35 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:35 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:40<00:11,  2.38s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:36 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:36 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:37 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:37 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:42<00:09,  2.31s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:38 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:38 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:39 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:39 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:44<00:06,  2.29s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:40 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:40 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:46<00:04,  2.22s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:42 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:42 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_
25/09/24 21:54:44 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:44 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:48<00:02,  2.27s/it]/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
25/09/24 21:54:45 WARN StringIndexerModel: Input column OMS_MARKET does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_LIFESTAGE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DWELL_CODE does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_ECOHORT_GROUP_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column FIXED_VIDEO_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column FIXED_DATA_PRICEPOINT_TIER_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column DEMOS_ARCHETYPE_DESC does not exist during transformation. Skip StringIndexerModel for this column.
25/09/24 21:54:45 WARN StringIndexerModel: Input column BOX_EQUIP_CLASS does not exist during transformation. Skip StringIndexerModel for this column.
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
  warnings.warn(
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: VectorUDT()
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2299: RuntimeWarning: divide by zero encountered in log
  n_samples * np.log(2 * np.pi * self.noise_variance_)
/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2300: RuntimeWarning: invalid value encountered in divide
  + residuals_sum_squares / self.noise_variance_

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:51<00:00,  2.32s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:51<00:00,  2.23s/it]
âœ… SHAP summary plot saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/shap_summary_classification.png
âœ… SHAP values saved to /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/shap_values_classification.csv
âœ… SHAP values computed successfully for best model (lightgbm)

AutoML pipeline completed successfully!
================================================================================
ğŸ¯ MODEL SELECTION SUMMARY
================================================================================

ğŸ“Š Overall Statistics:
   â€¢ Total models trained: 2
   â€¢ Default models selected: 2
   â€¢ Tuned models selected: 0
   â€¢ Hyperparameter optimization: âŒ Disabled

ğŸ›ï¸ Selection Criteria:
   â€¢ Minimum improvement threshold: 1.0%
   â€¢ Maximum overfitting gap: 5.0%
   â€¢ Statistical significance threshold: 5.0%

ğŸ” Individual Model Decisions:

   ğŸ“Š XGBOOST: DEFAULT MODEL SELECTED
      ğŸ“ˆ Default performance: 76.7300
      ğŸ¯ Tuned performance: N/A (no tuning performed)
      ğŸ’­ Key reasons:
         â€¢ Hyperparameter optimization is disabled

   ğŸ“Š LIGHTGBM: DEFAULT MODEL SELECTED
      ğŸ“ˆ Default performance: 53.0700
      ğŸ¯ Tuned performance: N/A (no tuning performed)
      ğŸ’­ Key reasons:
         â€¢ Hyperparameter optimization is disabled

================================================================================
ğŸ’¡ INTERPRETATION GUIDE:
   ğŸ¯ TUNED models: Hyperparameter optimization provided significant benefit
   ğŸ“Š DEFAULT models: Default parameters performed as well or better
   ğŸª Overfitting gap: Difference between training and validation performance
   ğŸ“ˆ Higher scores are better for accuracy, AUC, KS; lower for loss metrics
================================================================================

ğŸ“„ Model selection summary saved to: /tmp/automl_results/job_0002_automl_user_automl_model_1758749709/model_selection_summary.txt
[2025-09-24T21:54:47.717188] ğŸ“Š Progress: 75.0% - AutoML training completed
[2025-09-24T21:54:47.717219] âœ… AutoML.fit() completed successfully
[2025-09-24T21:54:47.717234] ğŸ“Š Progress: 87.5% - Saving results to GCS...
[2025-09-24T21:54:47.717244] ğŸ‰ AutoML pipeline completed successfully!
[2025-09-24T21:54:47.717256] ğŸ“¤ Copying results from /tmp/automl_results/job_0002_automl_user_automl_model_1758749709 to GCS...
[2025-09-24T21:54:54.190106] âœ… Results successfully copied to gs://rapid_modeler_app/automl_results/job_0002_automl_user_automl_model_1758749709/
[2025-09-24T21:54:54.191015] ğŸ“‹ Creating comprehensive execution logs for Streamlit...
[2025-09-24T21:54:54.191081] âš ï¸ Warning: Failed to create job status files: local variable 'sys' referenced before assignment
[2025-09-24T21:54:54.191108] ğŸ“Š Progress: 100.0% - Job completed successfully
[2025-09-24T21:54:54.191119] ğŸ‰ Dataproc Serverless job completed successfully!
[2025-09-24T21:54:54.191457] ğŸš¨ Emergency shutdown timer set: 10 seconds post-completion
[2025-09-24T21:54:54.191501] ğŸ‰ Dataproc Serverless job completed successfully!
[2025-09-24T21:54:54.191554] âš ï¸ Warning: Could not mark threads as daemon: cannot set daemon status of active thread
[2025-09-24T21:54:54.191574] ğŸ§¹ Closing Spark session (graceful)...
[2025-09-24T21:54:55.795348] âœ… Spark session closed successfully
[2025-09-24T21:54:55.795387] ğŸ“Œ Job completed successfully - initiating immediate termination to avoid hang
[2025-09-24T21:54:55.795400] ğŸšª Implementing enhanced force shutdown mechanism...
[2025-09-24T21:54:55.795652] ğŸšª Attempting graceful sys.exit...
[2025-09-24T21:54:55.795704] ğŸšª Attempting os._exit...
